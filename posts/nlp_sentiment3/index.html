<!doctype html>

<html lang="en">

<head>
  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-M72HGJH');</script>

  <title>NLP with distrowatch reviews. Part III: Sentiment classification - Diego Hernández Jiménez</title>
  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="description" content="Kiera: A Hugo theme for creative and technical writing." />
<meta name="author" content="Example" /><meta property="og:title" content="NLP with distrowatch reviews. Part III: Sentiment classification" />
<meta property="og:description" content="Description Some time ago, I contemplated transitioning to a Linux-based operating system, driven by my curiosity about the realm of free software and Linux culture. During my exploration, I stumbled upon the Distrowatch website (an invaluable resource offering information about numerous Linux distributions, including reviews)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment3/" />
<meta property="article:published_time" content="2023-08-08T00:00:00+02:00" />
<meta property="article:modified_time" content="2023-08-08T00:00:00+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NLP with distrowatch reviews. Part III: Sentiment classification"/>
<meta name="twitter:description" content="Description Some time ago, I contemplated transitioning to a Linux-based operating system, driven by my curiosity about the realm of free software and Linux culture. During my exploration, I stumbled upon the Distrowatch website (an invaluable resource offering information about numerous Linux distributions, including reviews)."/>

<meta name="generator" content="Hugo 0.74.3" />
    
    <link rel="shortcut icon" href="https://diego-hernandez-jimenez.github.io/web/images/Dicon.ico" />
  
    <script src="https://diego-hernandez-jimenez.github.io/web/js/mathjax-config.js" defer></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />
  <link rel="stylesheet" href="https://diego-hernandez-jimenez.github.io/web/fontawesome/css/all.min.css" />
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" />
  
  
  <link rel="stylesheet" type="text/css" href="https://diego-hernandez-jimenez.github.io/web/css/styles.css" />
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script type="text/javascript">
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            options: {
                processEscapes: true
            }
        };
    </script>
</head>

<body>
  <div id="container">
    <header>
      <h1>
                <a href="https://diego-hernandez-jimenez.github.io/web/">Diego Hernández Jiménez</a>
            </h1>

      <ul id="social-media">
             <li>
               <a href="https://github.com/Diego-Hernandez-Jimenez" title="GitHub">
               <i class="fab fa-github fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://linkedin.com/in/diego-hern%c3%a1ndez-jim%c3%a9nez" title="LinkedIn">
               <i class="fab fa-linkedin fa-lg"></i>
               </a>
             </li>
      </ul>
      
      <p><em>Welcome to my personal website! Here I share some of my little projects.</em></p>
      
    </header>

    
<nav>
    <ul>
        
        <li>
            <a class="" href="https://diego-hernandez-jimenez.github.io/web/tags">
                <i class="fa-li fa  fa-lg"></i><span>Tags</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://diego-hernandez-jimenez.github.io/web/posts/">
                <i class="fa-li fa  fa-lg"></i><span>Projects</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://diego-hernandez-jimenez.github.io/web/about/">
                <i class="fa-li fa  fa-lg"></i><span>About</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://diego-hernandez-jimenez.github.io/web/contact/">
                <i class="fa-li fa  fa-lg"></i><span>Contact</span>
            </a>
        </li>
        
    </ul>
</nav>


    <main>




<article>

    <h1>NLP with distrowatch reviews. Part III: Sentiment classification</h1>

    
      <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2023-08-08T00:00:00&#43;02:00">Aug 8, 2023</time>
        </li>
        
        <li>
            Categories:
            <em>
                
                    
                    <a href="https://diego-hernandez-jimenez.github.io/web/categories/projects">Projects</a>
                
            </em>
        </li>
        

        
        <li>
            <em>
                
                    
                    <a href="https://diego-hernandez-jimenez.github.io/web/tags/python">#Python</a>
                
                    , 
                    <a href="https://diego-hernandez-jimenez.github.io/web/tags/data-analysis">#Data Analysis</a>
                
                    , 
                    <a href="https://diego-hernandez-jimenez.github.io/web/tags/for-fun">#For fun</a>
                
            </em>
        </li>
        

        <li>6 minutes read</li>
    </ul>
</aside>

    

    


    <h3 id="description">Description</h3>
<p>Some time ago, I contemplated transitioning to a Linux-based operating system, driven by my curiosity about the realm of free software and Linux culture. During my exploration, I stumbled upon the <a href="https://distrowatch.com/">Distrowatch</a> website (an invaluable resource offering information about numerous Linux distributions, including reviews). The organized and very structured layout of these reviews, coupled with the inclusion of numerical scores, sparked an idea within me. I envisioned gathering a substantial collection of these reviews—ranging from tens to potentially thousands—and subjecting them to analysis through Natural Language Processing (NLP). Eventually, this idea materialized into a tangible project, the outcome of which can be accessed here.</p>
<p>The project consists of three parts: Web scraping, supervised and unsupervised learning for sentiment analysis. We&rsquo;ve already seen part I and II, let&rsquo;s end with unsupervised learning.</p>
<h2 id="problem-definition">Problem definition</h2>
<p>In Part II, we categorized ratings using our intuition in order to differentiate between positive and negative reviews. However, this approach has a significant drawback, as it heavily depends on the subjectivity of the analyst, which in this case is me. Moreover, it introduces the potential for bias. Consequently, we will now explore alternative strategies for defining our target variable, mainly using unsupervised techniques.</p>
<h2 id="agglomerative-clustering">Agglomerative clustering</h2>
<p>What if we were to group categories/ratings based on the similarity of their features? To achieve this, we can generate a <em>prototype</em> exemplar for each rating and then apply a hierarchical clustering algorithm to group these representative members of each category. One relatively straightforward (although potentially simplistic) approach to creating a <em>prototype</em> for a rating/class $K$ is by calculating the average value of each feature across all instances with rating $K$.</p>
<p>The result of applying hierarchical clustering using cosine similarity and complete linkage is the following:</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/dendro_nlp.png" alt="dendrogram"></p>
<p>Interestingly, with this solution, we find ourselves in a situation where it makes sense to create two groups, roughly corresponding to positive reviews (ratings 8-10) and negative reviews (ratings 1-7). However, it&rsquo;s worth noting that this classification is only approximate because intuitively, one might not consider a rating of 6 or 7 as entirely negative.</p>
<p>Using the two clusters as classes of the target variable, we can now train a model as we did in part II (same approach for text processing and data partition). Using Complement Naïve Bayes we get a global accuracy score of 0.844 and balanced accuracy of 0.828.</p>
<h2 id="k-means">K-means</h2>
<p>So far, in one way or another, the original rating data has been used to create the target variable. However, it is possible to take a completely unsupervised approach and generate the classes from the clusters that naturally emerge in the data. This can be achieved with K-means. Using this algorithm we can partition the dataset into a number of predefined groups and use the clusters ids as categories for supervised classification.</p>
<p>I decided to fit a model with two clusters because the elbow method didn&rsquo;t help me choose the number of groups.The resulting clusters were completely (and therefore our target variable) new so I thought it would be reasonable to repeat the process of cross-validation for model selection. This time I introduced new models in the competetion: logistic regression and linear Support Vector Machine (SVM). The 5-cross-validation average balanced accuracy scores are show next:</p>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Complement Naïve Bayes</td>
<td align="center">0.638</td>
</tr>
<tr>
<td align="left">Logistic regression</td>
<td align="center">0.876</td>
</tr>
<tr>
<td align="left">Support Vector Machine</td>
<td align="center">0.956</td>
</tr>
</tbody>
</table>
<p>Of course I chose SVM as the final model. In terms of test performance, this is what I got:</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/confmat_test_svm.png" alt="confusion_matrix"></p>
<p>Pretty amazing, right? What happened here? What do the clusters represent? The model can be extremely good discriminating between classes, but if we don&rsquo;t know the meaning of those classes, then the model is not useful at all. We hoped the categories could represent sentiment, is that really the case? Take a look at the distribution of ratings:</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/dist_by_cluster.png" alt="distribution_by_cluster"></p>
<p>Mmm&hellip; I&rsquo;m not sure. Yes, there is a higher proportion of low ratings (more evident with ratings of 1) in cluster 1 and a higher portion of very good reviews (8 or 9) in cluster 0, but the groups do not seem to be that distinct.</p>
<p>What about term frequency?</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/token_dist_km.png" alt="token_distribution_by_cluster"></p>
<p>Not very clear.</p>
<p>Finally, taking advantage of the fact that we fit a linear classifier (they are usually easier to interpret), I examined the coefficients of the SVM</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
coefs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(model[<span style="color:#e6db74">&#39;clf&#39;</span>]<span style="color:#f92672">.</span>coef_<span style="color:#f92672">.</span>todense())
cl0_coefs,cl1_coefs <span style="color:#f92672">=</span> coefs[coefs<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>],coefs[coefs<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span>]
top_cl0_tok,top_cl1_tok <span style="color:#f92672">=</span> cl0_coefs<span style="color:#f92672">.</span>argsort()[:<span style="color:#ae81ff">10</span>],np<span style="color:#f92672">.</span>flip(cl1_coefs<span style="color:#f92672">.</span>argsort()[<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>:])
toks <span style="color:#f92672">=</span> model[<span style="color:#e6db74">&#39;vectorizer&#39;</span>]<span style="color:#f92672">.</span>get_feature_names_out()

pd<span style="color:#f92672">.</span>DataFrame({
    <span style="color:#e6db74">&#39;token cluster 0&#39;</span>:toks[top_cl0_tok],
    <span style="color:#e6db74">&#39;coef cluster 0&#39;</span>:cl0_coefs[top_cl0_tok],
    <span style="color:#e6db74">&#39;token cluster 1&#39;</span>:toks[top_cl1_tok],
    <span style="color:#e6db74">&#39;coef cluster 1&#39;</span>:cl1_coefs[top_cl1_tok],
})
</code></pre></div><p><img src="https://diego-hernandez-jimenez.github.io/web/images/svm_coefs.png" alt="svm_coefficients"></p>
<p>This is more interesting. Words like <em>mail</em> or <em>adds</em> have a strong influence on the decision boundary, meaning that the model relies more on them to classify the review, specifically into cluster 0. Generally, negative coefficients exhibit greater magnitude, which implies an emphasis on the &ldquo;negative class&rdquo;, which is cluster 0. This results in a skewed decision boundary favoring this class, which happens to be the minority class, by the way.</p>
<p>Still, I find difficult to give meaning to the clusters.</p>
<h2 id="vader">VADER</h2>
<p>The last strategy to be considered involves the use of VADER (Valence Aware Dictionary and sEntiment Reasoner, consider checking the <a href="https://sensorpro.net/ml/vader.pdf">original paper</a> or the <a href="https://www.nltk.org/_modules/nltk/sentiment/vader.html">implementation in nltk</a>). This is a completely different approach, as VADER is a rule-based model that can itself be used as a classifier. It assigns scores to texts on a sentiment polarity scale ranging from -1 to 1 (-1 being the most negative, 1 being the most positive). Typically, a score of 0.5 or higher is considered to indicate positive sentiment, while a score of -0.5 or lower indicates negative sentiment, with the remaining scores reflecting neutrality.</p>
<p>When I applied it the first time I noticed a problem, however. It was clearly failing with many reviews. How do I know that? Well, remember that we had the real ratings given by the users. Now, let&rsquo;s take a look at the distribution of scores in each category resulting from applying VADER:</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/dist_by_vader.png" alt="distributions_by_vader_sent"></p>
<p>It doesn&rsquo;t seem logical that such a high proportion of ratings of 10 are classified as negative. Similarly, it&rsquo;s also puzzling to find extreme cases (scores of 1 or 10) in reviews categorized as neutral.</p>
<p>For this reason, I then adopted a mixed approach where I utilized VADER but also took into consideration the information provided by the ratings. Specifically, VADER was only applied to reviews with ratings between 3 and 8. For the rest, which were clearly positive or negative, their scores were scaled to fit within the range of [-1, 1], and then the <code>tag_review</code> function was applied (see below). For VADER-rated reviews, the final sentiment score didn&rsquo;t solely rely on VADER itself; it was a weighted combination of the VADER score and the scaled rating score:</p>
<p>$sentiment=\text{tag_review}(w\cdot VADER+(1-w)\cdot rating)$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tag_review</span>(compound_score):
  <span style="color:#66d9ef">if</span> compound_score <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span>:
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;pos&#39;</span>
  <span style="color:#66d9ef">elif</span> compound_score <span style="color:#f92672">&lt;=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>:
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;neg&#39;</span>
  <span style="color:#66d9ef">else</span>:
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;neu&#39;</span>

</code></pre></div><p>The new distributions seem more like what we would expect:</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/dist_by_vader2.png" alt="distributions_by_vader_sent"></p>
<p>Then I repeated the model selection step. The results of the final Complement Naïve Bayes are shown next:</p>
<p><img src="https://diego-hernandez-jimenez.github.io/web/images/confmat_test_vader.png" alt="confusion_matrix"></p>
<p>An unacceptable 68% of what we considered neutral reviews were classified as negative, and 24% were classified as positive reviews. This category caused considerable difficulty, so I considered reverting to the binary scenario. This was accomplished by simply altering the decision rule of VADER, that is, the <code>tag_review</code> function: scores $\geq$ 0 were positive, the rest, negative.</p>
<p>This lead to a new model that achieved 0.843 accuracy and 0.807 balanced accuracy, similar performance to what we saw earlier.</p>
<p>If you want to know more about the project, check the part I and II. All the code can be found <a href="https://github.com/Diego-Hernandez-Jimenez/nlp_distrowatch/tree/main">here</a>.</p>


</article>


<section class="post-nav">
    <ul>
        
        <li>
            <a href="https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment2/"><i class="fa fa-chevron-circle-left"></i> NLP with distrowatch reviews. Part II: Sentiment classification</a>
        </li>
        
        
        <li>
            <a href="https://diego-hernandez-jimenez.github.io/web/posts/lvq1/">Prototype-based learning. Part I: GMLVQ from scratch <i class="fa fa-chevron-circle-right"></i> </a>
        </li>
        
    </ul>
</section>
  
    
    
  





</main>
    <footer>
        <h6>Copyright &amp; copy; 2023 - Diego Hernández Jiménez |
            Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
            <a href="https://diego-hernandez-jimenez.github.io/web/index.xml">Subscribe </a></h6>
    </footer>
</div>
<script src="https://diego-hernandez-jimenez.github.io/web/js/scripts.js"></script>

</body>

</html>

