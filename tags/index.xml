
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
 <channel>
   <title>Tags on Diego Hernández Jiménez</title>
   <link>https://diego-hernandez-jimenez.github.io/web/tags/</link>
   <description>Recent content in Tags on Diego Hernández Jiménez</description>
   <generator>Hugo -- gohugo.io</generator>
   <copyright>Copyright &amp; copy; 2023 - Diego Hernández Jiménez</copyright>
   <lastBuildDate>Sat, 17 Aug 2024 00:00:00 +0200</lastBuildDate>
   
       <atom:link href="https://diego-hernandez-jimenez.github.io/web/tags/index.xml" rel="self" type="application/rss+xml" />
   
   
     <item>
       <title>Prototype-based learning. Part I: GMLVQ from scratch</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/lvq1/</link>
       <pubDate>Sat, 17 Aug 2024 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/lvq1/</guid>
       <description>&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;In my journey to bridge psychology and data science, I discovered Learning Vector Quantization (LVQ) and immediately saw its potential connection to human category learning. This realization led me to dive deeper, implementing LVQ from scratch, initially by coding all the operations using PyTorch, and then by abstracting the optimization and learning processes to fully leverage PyTorch&amp;rsquo;s features.&lt;/p&gt;
&lt;h3 id=&#34;problem&#34;&gt;Problem&lt;/h3&gt;
&lt;p&gt;Human category learning has been a subject of extensive study, with two predominant approaches: exemplar-based learning and prototype-based learning. Prototype learning, in particular, posits &amp;ldquo;that a category of things in the world (objects, animals, shapes, etc.) can be represented in the mind by a prototype. A prototype is a cognitive representation that captures the regularities and commonalities among category members and can help a perceiver distinguish category members from non-members&amp;rdquo; (Minda &amp;amp; Smith, 2011)&lt;/p&gt;
&lt;p&gt;And what about LVQ? Well, this model learns prototypes and classifies new examples based on comparisons with these prototypes, which makes it a very promising model for human learning.&lt;/p&gt;
&lt;h3 id=&#34;what-is-different-and-interesting&#34;&gt;What is different and interesting?&lt;/h3&gt;
&lt;p&gt;One of the compelling aspects of LVQ is its flexibility in learning prototypes. Unlike traditional prototype models, which often rely on a single, average prototype per category, LVQ allows for multiple prototypes per category. These prototypes are not merely averages but can be more complex summaries of the data.&lt;/p&gt;
&lt;p&gt;Furthermore, the similarity function in LVQ can be learned and adapted. Besides, this function doesn&amp;rsquo;t have to be unique; each prototype can have its own similarity function. This flexibility enables some LVQ models to handle not only linearly separable categories but also those that are non-linearly separable —a significant improvement over standard prototype models, which struggle with non-linear separations.&lt;/p&gt;
&lt;h4 id=&#34;objective&#34;&gt;Objective&lt;/h4&gt;
&lt;p&gt;While LVQ shows promise as a cognitive model, validating it against human experimental data is beyond my current means due to a lack of access to such data. Therefore, my focus will be on evaluating LVQ&amp;rsquo;s effectiveness as a machine learning classifier. In this first part, I aim to replicate the results of a foundational paper in the LVQ field—the very paper that introduced me to this family of models (Schneider et al., 2009).&lt;/p&gt;
&lt;h4 id=&#34;how-to-translate-the-model-into-code&#34;&gt;How to translate the model into code?&lt;/h4&gt;
&lt;p&gt;I&amp;rsquo;ve documented all the process in this report &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/lvq_scratch.pdf&#34;&gt;here&lt;/a&gt; and gathered all the code in  &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/prototype_learning_LVQ/blob/main/GMLVQ_from_scratch.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Essentially, a trained LVQ model works by comparing an unseen exemplar ${\bf x}$ to each of the learned prototypes stored in memory ${\bf w}_1,&amp;hellip;,{\bf w}_C$. There may be one for each of the $C$ classes, as here, or more than one. The comparison is done using a dissimilarity function $d$ defined as (assuming the use of row vectors instead of conventional column vectors):&lt;/p&gt;
&lt;p&gt;$$
d({\bf x},{\bf w}_k,{\bf Q})=({\bf x}-{\bf w}_k) {\bf R} ({\bf x}-{\bf w}_k)^\intercal
$$&lt;/p&gt;
&lt;p&gt;Here, ${\bf R}$ is relevance matrix. In GMLVQ, it&amp;rsquo;s a dense symmetric matrix of relevance/attention weights given to features  where $tr({\bf R}) = 1$. If diagonal, the model will be Relevance LVQ (RLVQ) and won&amp;rsquo;t account for correlations between features. If identity matrix, we have the Generalized LVQ (GLVQ). When there is one matrix per prototype, we are working with localized versions of the model. In this first part of the project only GMLVQ and LGMLVQ are implemented. Also, this relevance matrix (or matrices) are expressed as ${\bf R}={\bf Q}^\intercal{\bf Q}$, so the dissimilarity becomes:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
d({\bf x},{\bf w}_k,{\bf Q})&amp;amp;=({\bf x}-{\bf w}_k) {\bf Q}^\intercal{\bf Q} ({\bf x}-{\bf w}_k)^\intercal \ 
&amp;amp;=\lVert {\bf Q}({\bf x}-{\bf w}_k)^\intercal \rVert_2^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The decision rule is to assign to the exemplar the class corresponding to the closest prototype:&lt;/p&gt;
&lt;p&gt;$$
\hat c=\underset{k\in C}{\operatorname{argmin}}\ d({\bf x},{\bf w}_k,{\bf R})
$$&lt;/p&gt;
&lt;p&gt;LVQ models learn the prototypes and the relevance matrix (or matrices) using gradient descent by minimizing the following relative distance loss:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}(d^+,d^-)=\phi\Bigl( \frac{d^+-d^-}{d^++d^-} \Bigr)
$$&lt;/p&gt;
&lt;p&gt;where $\phi$ is a function that can add an additional transformation. Here we are going to assume, as in the paper I&amp;rsquo;m using as reference, that $\phi(x)=x$, but in the second part we&amp;rsquo;ll try some other functions such as the sigmoid or ReLU. Notice that we have $d^+$ and $d^-$. Those distances are the distance of the data point from the closest prototype with the &lt;em&gt;correct&lt;/em&gt; label (${\bf w}^+$) and the distance of the data point from the closest prototype with the &lt;em&gt;incorrect&lt;/em&gt; label (${\bf w}^-$), respectively (from the energy-based model learning point of view, the latter would correspond to the &amp;ldquo;most offending answer&amp;rdquo;, a name that I found quite funny). In the &amp;ldquo;localized&amp;rdquo; model scenario, we would also have ${\bf R}^+$ and ${\bf R}^-$.&lt;/p&gt;
&lt;p&gt;The tricky part when you implement a ML algorithm like this is that you need to code everything from scratch, including the derivatives (gradients) that are used for the parameter updates, so you need to understand very well how the model works. To save space, I won&amp;rsquo;t write here all the functions I created. If interested, check the report and/or notebook mentioned above.&lt;/p&gt;
&lt;h4 id=&#34;validation-with-artificial-data&#34;&gt;Validation with artificial data&lt;/h4&gt;
&lt;p&gt;After having programmed the model, we&amp;rsquo;re ready to test it. In this first part we&amp;rsquo;ve only used a fake dataset with the same properties as the one generated in Schneider et al. (2009). It looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/SchneiderGMLVQ_data.png&#34; alt=&#34;fake data with non-linearly separable classes&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we instantiate a GMLVQ model and train it for 25 epochs (way less that in the original paper, but there was no need of more epochs) we achieve around 0.8 accuracy which is pretty close to the one obtained in the paper. If we visualize the results we can see why tho model fails achieve higher accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/GMLVQ_decision_area.png&#34; alt=&#34;decision area for GMLVQ&#34;&gt;&lt;/p&gt;
&lt;p&gt;As said before (and much better explained in the paper, GMLVQ is at the end a linear classifier).&lt;/p&gt;
&lt;p&gt;However, when we allow multiple relavance matrices with LGMLVQ, we obtain 0.91 accuracy and this is what we get.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/LGMLVQ_decision_area.png&#34; alt=&#34;decision area for LGMLVQ&#34;&gt;&lt;/p&gt;
&lt;p&gt;Nice, just as expected!&lt;/p&gt;
&lt;p&gt;However, despite the success (I was very proud when I finally made it work), this was only the beginning, the code in this proof of concept was very rigid and didn&amp;rsquo;t allow for many changes. For that reason, just after I finished this I started working on a better version. Check it out in the &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/posts/lvq2/&#34;&gt;second part&lt;/a&gt;
!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Minda, J.P. &amp;amp; Smith, D. (2011). Prototype models of categorization: basic formulation, predictions, and limitations. In E.M. Pothos, N. Chater &amp;amp; P. Hines, &lt;em&gt;Formal Approaches in Categorization&lt;/em&gt; (pp. 40-65)&lt;/p&gt;
&lt;p&gt;Schneider, P., Biehl, M. &amp;amp; Hammer, B. (2009). Adaptive relevance matrices in Learning Vector
Quantization, &lt;em&gt;Neural computation&lt;/em&gt;, 21(12), 3532-3561.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Prototype-based learning. Part II: LVQ family of models in Pytorch</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/lvq2/</link>
       <pubDate>Sat, 17 Aug 2024 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/lvq2/</guid>
       <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
&lt;p&gt;Link to code &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/prototype_learning_LVQ/blob/main/LVQ_pytorch.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>NLP with distrowatch reviews. Part I: Web scraping</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment1/</link>
       <pubDate>Sun, 06 Aug 2023 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment1/</guid>
       <description>&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Some time ago, I contemplated transitioning to a Linux-based operating system, driven by my curiosity about the realm of free software and Linux culture. During my exploration, I stumbled upon the &lt;a href=&#34;https://distrowatch.com/&#34;&gt;Distrowatch&lt;/a&gt; website (an invaluable resource offering information about numerous Linux distributions, including reviews). The organized and very structured layout of these reviews, coupled with the inclusion of numerical scores, sparked an idea within me. I envisioned gathering a substantial collection of these reviews—ranging from tens to potentially thousands—and subjecting them to analysis through Natural Language Processing (NLP). Eventually, this idea materialized into a tangible project, the outcome of which can be accessed here.&lt;/p&gt;
&lt;p&gt;The project consists of three parts: Web scraping, supervised and unsupervised learning for sentiment analysis. Let&amp;rsquo;s start with web scraping and data collection.&lt;/p&gt;
&lt;h2 id=&#34;inspecting-the-website&#34;&gt;Inspecting the website&lt;/h2&gt;
&lt;p&gt;If we go to this site &lt;code&gt;https://distrowatch.com/dwres.php?resource=ratings&amp;amp;distro={distro_of_choice}&lt;/code&gt;, we&amp;rsquo;ll see reviews like this one:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/ej_review_distro.png&#34; alt=&#34;review_example&#34;&gt;&lt;/p&gt;
&lt;p&gt;Just by seeing it, one has the feeling that the underlying html is going to be clearly structured and that is going to be relatively easy to access all of its parts. This is in fact what we see when we inspect elements:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/review_inspect.png&#34; alt=&#34;review_html&#34;&gt;&lt;/p&gt;
&lt;p&gt;The code I used to extract each part is relatively simple, although, to be honest, I needed quite some time to get it because I&amp;rsquo;m not at all expert in web scraping.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;total_reviews &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[1]//td[2]/b[2]/text()&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
avg_rating &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[1]//td[2]/div/text()&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
project &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[2]//tr/td[1]/text()[3]&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
version &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[2]//tr/td[1]/text()[4]&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
rating &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[2]//tr/td[1]/text()[5]&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
date_review &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[2]//tr/td[1]/text()[6]&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
votes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[2]//tr/td[1]/text()[7]&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()
reviews &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xpath(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;//td[@class = &amp;#34;News1&amp;#34;]//table[2]//tr/td[2]/text()&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There was a little problem, however. Notice the space between paragraphs in the hmtl from picture above? That was inconvenient because in the extracted reviews, different paragraphs were separated by &lt;code&gt;/r&lt;/code&gt;, but different reviews are separated by \n. The subparts of the review (paragraphs) had to be joined together. Also, there was another &lt;code&gt;\n&lt;/code&gt; character at the beggining of the next review. In order to split the reviews correctly I firstly &amp;ldquo;glued&amp;rdquo; all reviews and then used the pattern &amp;lsquo;\n\n&amp;rsquo; to separate texts.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# the first reviews before processing&lt;/span&gt;

[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Ubuntu was the first distribution I used on Linux. Then I was a distrohopper for 4 years and learned everything about Debian/Ubuntu, Arch, Gentoo, Opensuse and other alternatives like Void, Slakware, etc.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I always come back to Ubuntu. I don&amp;#39;t like snaps or the continuous error in the snap-store, but I feel that it is the most stable, modern and efficient distro.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;When I want to learn about Linux, I use other distributions, but when I want to work with Linux, I always go back to Ubuntu. I use flatpak because I like it better, nobody prevents it even if you don&amp;#39;t have flatpak pre-installed.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Oh! and Ubuntu is the only distro I&amp;#39;ve installed on all types of hardware (old imac, new hardware, old hardware) and it always works. It just all works.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;I have used Ubuntu Linux for years, and I have had good experiences with it most time.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Firstly, it&amp;#39;s user friendly. For someone switching from Windows to Linux, Ubuntu is a good start, not with a deep learning curve.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;,
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Secondly, it&amp;#39;s supported by many popular software: alternative web browsers, VPN client tools, RDP tools, software development tools, etc.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;reviews &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(reviews)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(reviews) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; len(rating) &lt;span style=&#34;color:#75715e&#34;&gt;# to maje sure now we have the correct number of reviews&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And that was pretty much all. I also retrieved the names of all distributions from other page of the web and faced other problems (limited number of reviews that could be accessed), but the heavy part was that.&lt;/p&gt;
&lt;h2 id=&#34;dataset-creation&#34;&gt;Dataset creation&lt;/h2&gt;
&lt;p&gt;To create a dataset for posterior analysis I choose a simple method. I basically iterated over all the distros available and created a dataframe for each one with their information. Then I just concatenated all the dataframes. Each dataframe was defined as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df_distro &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;:date_review,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;project&amp;#39;&lt;/span&gt;:project,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;version&amp;#39;&lt;/span&gt;:version,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rating&amp;#39;&lt;/span&gt;:rating,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;votes&amp;#39;&lt;/span&gt;:votes,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;review&amp;#39;&lt;/span&gt;:reviews})

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the final &lt;code&gt;df&lt;/code&gt; I made some modifications to avoid problems later and I saved it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
    df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;assign(date&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_datetime(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;]),
              votes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_numeric(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;votes&amp;#39;&lt;/span&gt;]),
              rating&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_numeric(df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rating&amp;#39;&lt;/span&gt;]))
    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_index(drop&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
)

df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;distrowatch.csv&amp;#39;&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s how a review looked:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/review_final.png&#34; alt=&#34;processed_review&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you want to know more about the project, check the part II and III. All the code can be found &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/nlp_distrowatch/tree/main&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>NLP with distrowatch reviews. Part II: Sentiment classification</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment2/</link>
       <pubDate>Sun, 06 Aug 2023 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment2/</guid>
       <description>&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Some time ago, I contemplated transitioning to a Linux-based operating system, driven by my curiosity about the realm of free software and Linux culture. During my exploration, I stumbled upon the &lt;a href=&#34;https://distrowatch.com/&#34;&gt;Distrowatch&lt;/a&gt; website (an invaluable resource offering information about numerous Linux distributions, including reviews). The organized and very structured layout of these reviews, coupled with the inclusion of numerical scores, sparked an idea within me. I envisioned gathering a substantial collection of these reviews—ranging from tens to potentially thousands—and subjecting them to analysis through Natural Language Processing (NLP). Eventually, this idea materialized into a tangible project, the outcome of which can be accessed here.&lt;/p&gt;
&lt;p&gt;The project consists of three parts: Web scraping, supervised and unsupervised learning for sentiment analysis. We&amp;rsquo;ve already seen part I, let&amp;rsquo;s continue with sentiment analysis.&lt;/p&gt;
&lt;h2 id=&#34;problem-definition&#34;&gt;Problem definition&lt;/h2&gt;
&lt;p&gt;The idea is to predict the rating given by a user using only his/her review as input. However, this might not be as simple as it sounds, We need to clarify one basic assumption: we&amp;rsquo;re going to treat the prediction problem as a classification problem. If we had ratings with scores on a continuous scale with a greater range like 1-100, instead of 1-10, we could think about regression. Another thing to make clear is that, by default, we&amp;rsquo;re going to ignore the order present in ratings (i.e.: 10&amp;gt;8). In the notebook I explore one option to take order into account.&lt;/p&gt;
&lt;h2 id=&#34;basic-data-exploration&#34;&gt;Basic data exploration&lt;/h2&gt;
&lt;p&gt;Usually, we&amp;rsquo;d first split the data but because there is no risk of data leakage (I think), we can take a look at the distribution of ratings in the full dataset:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/class_balance.png&#34; alt=&#34;class_distribution&#34;&gt;&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve got 10 unique values and a very skewed distribution, most of the reviews we&amp;rsquo;ve collected are above 8. This translates into the problem of class imbalance.I don&amp;rsquo;t think I have enough expertise or data to treat this problem properly, so we&amp;rsquo;re going to make another assumption to simplify the problem. Given the relative prominence of extreme ratings, we could maybe assume scores are representing some underlying sentiment: negative, positive and maybe neutral. This means that there subsets of scores that belong to the same category (sentiment), but how can we group ratings? Here I&amp;rsquo;ll take a naïve approach and follow &amp;ldquo;common sense&amp;rdquo; (in part III I try other strategies). Here I show three possible options.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/target_encodings.png&#34; alt=&#34;target_encodings&#34;&gt;&lt;/p&gt;
&lt;p&gt;The first one seems reasonable and let us use binary classification algorithms (in the notebook I compare the performance using different target encoding)&lt;/p&gt;
&lt;h2 id=&#34;text-pre-processing&#34;&gt;Text pre-processing&lt;/h2&gt;
&lt;p&gt;Now we&amp;rsquo;re entering the realm of NLP. This is one of my first projects using NLP techniques, so I&amp;rsquo;ll keep it simple.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tokenization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The tokens (&amp;ldquo;building blocks&amp;rdquo;) of the reviews are the words, more especifically unigrams and bigrams, that is, &lt;em&gt;frustrating&lt;/em&gt; can be part of the vocabulary, but also the bigram &lt;em&gt;very frustrating&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Uppercase words are respected in some cases (not acronyms, for instance), because they can denote emphasis.&lt;/li&gt;
&lt;li&gt;Digits cannot be tokens (software versions, for example, are not deemed non-informative) except when presented in a format like &lt;em&gt;8/10&lt;/em&gt;, because explicit ratings can be very discriminative.&lt;/li&gt;
&lt;li&gt;Neither stemming nor lemmatization was applied. As I said, I wanted to keep it simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;preproc_tokenize&lt;/span&gt;(review):
  text_rating &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;findall(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\d\/10&amp;#39;&lt;/span&gt;,review)
  review_modif &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(\b[A-Z]{4,}\b)&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;upperc\1&amp;#39;&lt;/span&gt;,review)
  review_modif &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\d+\.?&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;,review_modif)
  tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;findall(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(?u)\b\w\w+\b|[!|?]&amp;#39;&lt;/span&gt;,review_modif&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower()) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; text_rating
  &lt;span style=&#34;color:#75715e&#34;&gt;# stemmed_tokens = [SnowballStemmer(&amp;#39;english&amp;#39;).stem(w) for w in tokens]&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tokens

ex &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;xdistrox version 2.04 is a great distro! 8/10. It works in my OS VERY nicely. Why nobody is talking about it??&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(ex)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(preproc_tokenize(ex))
  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;xdistrox version 2.04 is a great distro! 8/10. It works in my OS VERY nicely. Why nobody is talking about it??
[&#39;xdistrox&#39;, &#39;version&#39;, &#39;is&#39;, &#39;great&#39;, &#39;distro&#39;, &#39;!&#39;, &#39;it&#39;, &#39;works&#39;, &#39;in&#39;, &#39;my&#39;, &#39;os&#39;, &#39;uppercvery&#39;, &#39;nicely&#39;, &#39;why&#39;, &#39;nobody&#39;, &#39;is&#39;, &#39;talking&#39;, &#39;about&#39;, &#39;it&#39;, &#39;?&#39;, &#39;?&#39;, &#39;8/10&#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Stop words&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some specific words of this context are removed: &lt;em&gt;OS&lt;/em&gt;, &lt;em&gt;distro&lt;/em&gt;, &lt;em&gt;version&lt;/em&gt;, the name of the distributions&amp;hellip;The idea is to reduce the vocabulary size by discarding (supposedly) non-informative words.&lt;/li&gt;
&lt;li&gt;I keep some words like &lt;em&gt;very&lt;/em&gt; or &lt;em&gt;most&lt;/em&gt;, because they can be useful when expessing emotions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
to_keep &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;all&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;any&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;both&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;each&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;few&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;most&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;more&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;only&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;too&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;very&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;# not&lt;/span&gt;
stop &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [w &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; stopwords&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;words(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; to_keep]

extra_stop &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;OS&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;distro&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;LTS&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;version&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Linux&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;USB&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;PC&amp;#39;&lt;/span&gt;]

&lt;span style=&#34;color:#75715e&#34;&gt;# names were obtained from distrowatch.com using web scraping, see notebook for part 1&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;distro_names&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
  distro_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load(f)

stop&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend(extra_stop)
stop&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend(distro_names)

&lt;span style=&#34;color:#75715e&#34;&gt;# stopwords need to be processed as any other token&lt;/span&gt;
stop_tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; preproc_tokenize(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(stop))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vectorization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bag of Words: reviews are represented as vectors of counts with dimension equal to the total number of tokens in the dataset: the vocabulary size. The $i$-th entry of the vector for review $j$ stores the frequency of the $i$-th token of the vocabulary in the review $j$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Our Bag of Words vectorizer&lt;/span&gt;
bow_vectorizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CountVectorizer(
    lowercase&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False, &lt;span style=&#34;color:#75715e&#34;&gt;# transformation to lowercase is done in preproc_tokenize&lt;/span&gt;
    tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;preproc_tokenize, &lt;span style=&#34;color:#75715e&#34;&gt;# custom preprocessing and tokenizing function&lt;/span&gt;
    stop_words&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;stop_tokens, &lt;span style=&#34;color:#75715e&#34;&gt;# custom set of stop words&lt;/span&gt;
    token_pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, &lt;span style=&#34;color:#75715e&#34;&gt;# ignore, preproc_tokenize takes care&lt;/span&gt;
    ngram_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), &lt;span style=&#34;color:#75715e&#34;&gt;# unigrams and bigrams&lt;/span&gt;
    analyzer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;word&amp;#39;&lt;/span&gt;,
    max_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;,
    min_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# tokens have to appear at least in 20 documents -&amp;gt; this reduces the number of features drastically&lt;/span&gt;
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our token distribution looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/token_dist.png&#34; alt=&#34;token_distribution_by_class&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;model-training-and-selection&#34;&gt;Model training and selection&lt;/h2&gt;
&lt;p&gt;For this section I haven&amp;rsquo;t done a careful search to find the best algorithm. I wanted to try the Naïve Bayes model because its simplicity and because it doesn&amp;rsquo;t require hyper-parameter tuning.&lt;/p&gt;
&lt;p&gt;Classification rule: $\hat c=\underset{c\in K}{\operatorname{argmax}} P(c)\prod_{i=1}^{|V|} P(w_i|c)$&lt;/p&gt;
&lt;p&gt;Where $c$ represents each of the $k$ classes, $w_i$ is the $i$-th token of the vocabulary set $V$, with size $|V|$&lt;/p&gt;
&lt;p&gt;I compared the basic version, Multinomial Naïve Bayes Classifier (MNB), and the Complement Naïve Bayes Classifier (CNB), which is something like a improved version of MNB. I performed 5-cross-validation and used balanced accuracy for model selection, because it&amp;rsquo;s less misguiding in the presence of class imbalance than accuracy (or even AUC, as I&amp;rsquo;ve seen in other projects). With an average balanced accuracy of 0.813, CNB beat MNB, which obtained 0.801, so I chose the former as the final model.&lt;/p&gt;
&lt;p&gt;However, those are cross-validation performance scores, and we need to see how well the final model performs on unseen data. The results for the test set are the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/confmat_test.png&#34; alt=&#34;confusion_matrix&#34;&gt;&lt;/p&gt;
&lt;p&gt;With global accuracy of 0.867 and balanced accuracy of 0.85.&lt;/p&gt;
&lt;p&gt;Not bad, but far from excellent, a non-negiglible 18% of &amp;ldquo;negative&amp;rdquo; reviews are predicted as positive (maybe because of reviews on the border, those with rating of 5 or 6).&lt;/p&gt;
&lt;h2 id=&#34;which-tokens-are-most-relevant-for-classification&#34;&gt;Which tokens are most relevant for classification?&lt;/h2&gt;
&lt;p&gt;The probability of a class is given by the review $d$ is&lt;/p&gt;
&lt;p&gt;$P(c|d)=P(c)\prod_{i=1}^{|V_d|} P(w_i|c)$&lt;/p&gt;
&lt;p&gt;where $w_i$ now represents the tokens present in review $d$ with vocabulary $V_d$, a subset of $V$&lt;/p&gt;
&lt;p&gt;If we take the log:&lt;/p&gt;
&lt;p&gt;$\log P(c|d)=\log P(c)+\sum_{i=1}^{|V_d|} \log P(w_i|c)$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s easier to see now that, as happens with linear regression, the bigger the value of $\log P(w_i|c)$, the greater importance the token $w_i$ has for predictions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;top_tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
neg_prob_sorted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; final_model[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;feature_log_prob_[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, :]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort()[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
pos_prob_sorted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; final_model[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;feature_log_prob_[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, :]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort()[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;negative reviews&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(final_model[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vectorizer&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_feature_names_out(), neg_prob_sorted[:top_tokens]),&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;positive reviews&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(final_model[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vectorizer&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_feature_names_out(), pos_prob_sorted[:top_tokens]))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;negative reviews
[&#39;worst&#39; &#39;sad&#39; &#39;terrible&#39; &#39;tried install&#39; &#39;way too&#39; &#39;shame&#39; &#39;asks&#39;
 &#39;asking&#39; &#39;pass&#39; &#39;previous versions&#39; &#39;problematic&#39; &#39;tries&#39; &#39;failure&#39;
 &#39;parts&#39; &#39;get things&#39;] 

positive reviews
[&#39;configurable&#39; &#39;pleased&#39; &#39;charm&#39; &#39;well done&#39; &#39;snapshots&#39; &#39;upgraded&#39;
 &#39;intel core&#39; &#39;enjoying&#39; &#39;unix&#39; &#39;underrated&#39; &#39;impressive&#39; &#39;period&#39;
 &#39;most stable&#39; &#39;much easier&#39; &#39;favourite&#39;]
 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Very revealing indeed. For example, when a user mentions problems with the installation process or mentions previous versions of the software, the review is probably going to be negative. On the other hand, comments about how configurable and stable is the linux distribution predict positive reviews.&lt;/p&gt;
&lt;p&gt;If you want to know more about the project, check the part I and III. All the code can be found &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/nlp_distrowatch/tree/main&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>NLP with distrowatch reviews. Part III: Sentiment classification</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment3/</link>
       <pubDate>Sun, 06 Aug 2023 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/nlp_sentiment3/</guid>
       <description>&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Some time ago, I contemplated transitioning to a Linux-based operating system, driven by my curiosity about the realm of free software and Linux culture. During my exploration, I stumbled upon the &lt;a href=&#34;https://distrowatch.com/&#34;&gt;Distrowatch&lt;/a&gt; website (an invaluable resource offering information about numerous Linux distributions, including reviews). The organized and very structured layout of these reviews, coupled with the inclusion of numerical scores, sparked an idea within me. I envisioned gathering a substantial collection of these reviews—ranging from tens to potentially thousands—and subjecting them to analysis through Natural Language Processing (NLP). Eventually, this idea materialized into a tangible project, the outcome of which can be accessed here.&lt;/p&gt;
&lt;p&gt;The project consists of three parts: Web scraping, supervised and unsupervised learning for sentiment analysis. We&amp;rsquo;ve already seen part I and II, let&amp;rsquo;s end with unsupervised learning.&lt;/p&gt;
&lt;h2 id=&#34;problem-definition&#34;&gt;Problem definition&lt;/h2&gt;
&lt;p&gt;In Part II, we categorized ratings using our intuition in order to differentiate between positive and negative reviews. However, this approach has a significant drawback, as it heavily depends on the subjectivity of the analyst, which in this case is me. Moreover, it introduces the potential for bias. Consequently, we will now explore alternative strategies for defining our target variable, mainly using unsupervised techniques.&lt;/p&gt;
&lt;h2 id=&#34;agglomerative-clustering&#34;&gt;Agglomerative clustering&lt;/h2&gt;
&lt;p&gt;What if we were to group categories/ratings based on the similarity of their features? To achieve this, we can generate a &lt;em&gt;prototype&lt;/em&gt; exemplar for each rating and then apply a hierarchical clustering algorithm to group these representative members of each category. One relatively straightforward (although potentially simplistic) approach to creating a &lt;em&gt;prototype&lt;/em&gt; for a rating/class $K$ is by calculating the average value of each feature across all instances with rating $K$.&lt;/p&gt;
&lt;p&gt;The result of applying hierarchical clustering using cosine similarity and complete linkage is the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/dendro_nlp.png&#34; alt=&#34;dendrogram&#34;&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, with this solution, we find ourselves in a situation where it makes sense to create two groups, roughly corresponding to positive reviews (ratings 8-10) and negative reviews (ratings 1-7). However, it&amp;rsquo;s worth noting that this classification is only approximate because intuitively, one might not consider a rating of 6 or 7 as entirely negative.&lt;/p&gt;
&lt;p&gt;Using the two clusters as classes of the target variable, we can now train a model as we did in part II (same approach for text processing and data partition). Using Complement Naïve Bayes we get a global accuracy score of 0.844 and balanced accuracy of 0.828.&lt;/p&gt;
&lt;h2 id=&#34;k-means&#34;&gt;K-means&lt;/h2&gt;
&lt;p&gt;So far, in one way or another, the original rating data has been used to create the target variable. However, it is possible to take a completely unsupervised approach and generate the classes from the clusters that naturally emerge in the data. This can be achieved with K-means. Using this algorithm we can partition the dataset into a number of predefined groups and use the clusters ids as categories for supervised classification.&lt;/p&gt;
&lt;p&gt;I decided to fit a model with two clusters because the elbow method didn&amp;rsquo;t help me choose the number of groups.The resulting clusters were completely (and therefore our target variable) new so I thought it would be reasonable to repeat the process of cross-validation for model selection. This time I introduced new models in the competetion: logistic regression and linear Support Vector Machine (SVM). The 5-cross-validation average balanced accuracy scores are show next:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Complement Naïve Bayes&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.638&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Logistic regression&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Support Vector Machine&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.956&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Of course I chose SVM as the final model. In terms of test performance, this is what I got:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/confmat_test_svm.png&#34; alt=&#34;confusion_matrix&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pretty amazing, right? What happened here? What do the clusters represent? The model can be extremely good discriminating between classes, but if we don&amp;rsquo;t know the meaning of those classes, then the model is not useful at all. We hoped the categories could represent sentiment, is that really the case? Take a look at the distribution of ratings:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/dist_by_cluster.png&#34; alt=&#34;distribution_by_cluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mmm&amp;hellip; I&amp;rsquo;m not sure. Yes, there is a higher proportion of low ratings (more evident with ratings of 1) in cluster 1 and a higher portion of very good reviews (8 or 9) in cluster 0, but the groups do not seem to be that distinct.&lt;/p&gt;
&lt;p&gt;What about term frequency?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/token_dist_km.png&#34; alt=&#34;token_distribution_by_cluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not very clear.&lt;/p&gt;
&lt;p&gt;Finally, taking advantage of the fact that we fit a linear classifier (they are usually easier to interpret), I examined the coefficients of the SVM&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
coefs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(model[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;coef_&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;todense())
cl0_coefs,cl1_coefs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; coefs[coefs&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],coefs[coefs&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
top_cl0_tok,top_cl1_tok &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cl0_coefs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort()[:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;],np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flip(cl1_coefs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort()[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;:])
toks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vectorizer&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_feature_names_out()

pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame({
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;token cluster 0&amp;#39;&lt;/span&gt;:toks[top_cl0_tok],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;coef cluster 0&amp;#39;&lt;/span&gt;:cl0_coefs[top_cl0_tok],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;token cluster 1&amp;#39;&lt;/span&gt;:toks[top_cl1_tok],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;coef cluster 1&amp;#39;&lt;/span&gt;:cl1_coefs[top_cl1_tok],
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/svm_coefs.png&#34; alt=&#34;svm_coefficients&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is more interesting. Words like &lt;em&gt;mail&lt;/em&gt; or &lt;em&gt;adds&lt;/em&gt; have a strong influence on the decision boundary, meaning that the model relies more on them to classify the review, specifically into cluster 0. Generally, negative coefficients exhibit greater magnitude, which implies an emphasis on the &amp;ldquo;negative class&amp;rdquo;, which is cluster 0. This results in a skewed decision boundary favoring this class, which happens to be the minority class, by the way.&lt;/p&gt;
&lt;p&gt;Still, I find difficult to give meaning to the clusters.&lt;/p&gt;
&lt;h2 id=&#34;vader&#34;&gt;VADER&lt;/h2&gt;
&lt;p&gt;The last strategy to be considered involves the use of VADER (Valence Aware Dictionary and sEntiment Reasoner, consider checking the &lt;a href=&#34;https://sensorpro.net/ml/vader.pdf&#34;&gt;original paper&lt;/a&gt; or the &lt;a href=&#34;https://www.nltk.org/_modules/nltk/sentiment/vader.html&#34;&gt;implementation in nltk&lt;/a&gt;). This is a completely different approach, as VADER is a rule-based model that can itself be used as a classifier. It assigns scores to texts on a sentiment polarity scale ranging from -1 to 1 (-1 being the most negative, 1 being the most positive). Typically, a score of 0.5 or higher is considered to indicate positive sentiment, while a score of -0.5 or lower indicates negative sentiment, with the remaining scores reflecting neutrality.&lt;/p&gt;
&lt;p&gt;When I applied it the first time I noticed a problem, however. It was clearly failing with many reviews. How do I know that? Well, remember that we had the real ratings given by the users. Now, let&amp;rsquo;s take a look at the distribution of scores in each category resulting from applying VADER:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/dist_by_vader.png&#34; alt=&#34;distributions_by_vader_sent&#34;&gt;&lt;/p&gt;
&lt;p&gt;It doesn&amp;rsquo;t seem logical that such a high proportion of ratings of 10 are classified as negative. Similarly, it&amp;rsquo;s also puzzling to find extreme cases (scores of 1 or 10) in reviews categorized as neutral.&lt;/p&gt;
&lt;p&gt;For this reason, I then adopted a mixed approach where I utilized VADER but also took into consideration the information provided by the ratings. Specifically, VADER was only applied to reviews with ratings between 3 and 8. For the rest, which were clearly positive or negative, their scores were scaled to fit within the range of [-1, 1], and then the &lt;code&gt;tag_review&lt;/code&gt; function was applied (see below). For VADER-rated reviews, the final sentiment score didn&amp;rsquo;t solely rely on VADER itself; it was a weighted combination of the VADER score and the scaled rating score:&lt;/p&gt;
&lt;p&gt;$sentiment=\text{tag_review}(w\cdot VADER+(1-w)\cdot rating)$&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tag_review&lt;/span&gt;(compound_score):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; compound_score &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pos&amp;#39;&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; compound_score &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;neg&amp;#39;&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;neu&amp;#39;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The new distributions seem more like what we would expect:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/dist_by_vader2.png&#34; alt=&#34;distributions_by_vader_sent&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then I repeated the model selection step. The results of the final Complement Naïve Bayes are shown next:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/confmat_test_vader.png&#34; alt=&#34;confusion_matrix&#34;&gt;&lt;/p&gt;
&lt;p&gt;An unacceptable 68% of what we considered neutral reviews were classified as negative, and 24% were classified as positive reviews. This category caused considerable difficulty, so I considered reverting to the binary scenario. This was accomplished by simply altering the decision rule of VADER, that is, the &lt;code&gt;tag_review&lt;/code&gt; function: scores $\geq$ 0 were positive, the rest, negative.&lt;/p&gt;
&lt;p&gt;This lead to a new model that achieved 0.843 accuracy and 0.807 balanced accuracy, similar performance to what we saw earlier.&lt;/p&gt;
&lt;p&gt;If you want to know more about the project, check the part I and II. All the code can be found &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/nlp_distrowatch/tree/main&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>SQL and Python to explore IMDb movies</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/sql_imdb/</link>
       <pubDate>Mon, 12 Sep 2022 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/sql_imdb/</guid>
       <description>&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Knowing SQL is a must if you want to become a data scientist. From what I&amp;rsquo;ve seen in LinkedIn data-related job posts, the majority of employers are looking for people skilled in the use of SQL. I started learning SQL in 2020, but up to date I&amp;rsquo;ve never completed a project involving only SQL.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m changing that now. In the present project I&amp;rsquo;ve created a database from the &lt;a href=&#34;https://www.imdb.com/interfaces/&#34;&gt;imdb datasets&lt;/a&gt; using SQLite. I&amp;rsquo;ve written some queries to answer some basic but nonetheless interesting questions about cinema (e.g. which decade of the 20th century has movies with better ratings on average?) and finally I&amp;rsquo;ve translated some of this results into plots with the help of python and seaborn. To make things more challenging for me, I&amp;rsquo;ve developed the first part of the project (gathering the data and creating the database) using only the command line interface in a ubuntu &amp;ldquo;virtual machine&amp;rdquo; (the Windows Subsytem for Linux to be precise), without relying on GUIs.&lt;/p&gt;
&lt;p&gt;Here is the &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/imdb_insights.pdf&#34;&gt;executive report&lt;/a&gt; I made showing all the project stages.&lt;/p&gt;
&lt;h3 id=&#34;an-example-stanley-kubricks-filmography&#34;&gt;An example: Stanley Kubrick&amp;rsquo;s filmography&lt;/h3&gt;
&lt;p&gt;To get the films and ratings I had to use all 5 tables contained in the database (more info about each table in the imdb link):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; title, titleType, startYear &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;year&lt;/span&gt;, averageRating &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; rating
&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; crew
  &lt;span style=&#34;color:#66d9ef&#34;&gt;INNER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;names&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; crew.directors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;names&lt;/span&gt;.nconst
  &lt;span style=&#34;color:#66d9ef&#34;&gt;INNER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; ratings &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; crew.tconst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ratings.tconst
  &lt;span style=&#34;color:#66d9ef&#34;&gt;INNER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; akas &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; ratings.tconst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; akas.titleId
  &lt;span style=&#34;color:#66d9ef&#34;&gt;LEFT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; basics &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; akas.titleId &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; basics.tconst &lt;span style=&#34;color:#75715e&#34;&gt;-- left join to avoid losing data
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; primaryName &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Stanley Kubrick&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; isOriginalTitle &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; 
&lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;year&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;ASC&lt;/span&gt;;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The resulting table had missing data as can be seen in the next image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/kubrick_titles.jpg&#34; alt=&#34;table_kubrick_titles&#34;&gt;&lt;/p&gt;
&lt;p&gt;But at least it allowed me to make a simple plot showing the evolution of the director in terms of ratings and popularity (number of votes is used as a proxy for popularity)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/kubrick_ratings.jpg&#34; alt=&#34;kubrick_ratings&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/kubrick_votes.jpg&#34; alt=&#34;kubrick_votes&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is just a snippet. If you want to see more check my github repository &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/sql_imdb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Recognizing facial expression with eigenfaces</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/eigenfaces/</link>
       <pubDate>Sun, 12 Jun 2022 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/eigenfaces/</guid>
       <description>&lt;p&gt;Building of a simple facial expression recognition system based on Singular Value Decomposition.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Convolutional neural networks are the state of the art in virtually all tasks related with object recognition. However it&amp;rsquo;s worth visiting some of the older techniques used in this area.&lt;/p&gt;
&lt;p&gt;In 1991, Turk &amp;amp; Pentland proposed the &amp;ldquo;eigenface&amp;rdquo; approach, which is based on matrix factorization, specifically the Singular Value Decomposition (SVD), one the fundamental matrix factorization techniques in linear algebra. And also one of the most widely used.&lt;/p&gt;
&lt;p&gt;In this project I&amp;rsquo;ve tried to implement that strategy. I&amp;rsquo;ve added an additional goal. The aim is to recognize facial expressions, not just faces as a whole.The dataset I&amp;rsquo;m using for this purpose was created by Ebner et al. (2010).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve covered the details in this &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/eigfaces_R.pdf&#34;&gt;report&lt;/a&gt;, but the main ideas are the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit the photos. Here that means converting them to grayscale (so that when we transform them in matrices we have only two dimensions), reducing the resolution (smaller images, less computational cost) and cropping them (in order to remove uninteresting parts, like the background or the hair).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/origVSproc.jpg&#34; alt=&#34;original vs processed&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;Reshape each image into a p $\times$ 1 vector (p = width · height). We can arrange the n pictures to form the matrix ${\bf X}_{p\times n}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Split the data into train and test set. In this case, every person took two photos. We have a version of every facial expression of each person, so the split is easy and we get two equivalent sets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perform SVD (details in the report) on the training data. In essence, it will decompose the original matrix into 3 matrices. In this case, the first one contains the eigenvectors (eigenfaces). &amp;ldquo;These eigenvectors can be thought of as a set of features that together characterize the variation between face images.&amp;rdquo; (Turk &amp;amp; Pentland, 1991). &amp;ldquo;Some principal components may capture the most common features shared among all human faces, while other principal components will be more useful for distinguishing between individuals. Additional principal components may capture differences in lighting angles.&amp;rdquo; (Brunton &amp;amp; Kutz, 2020). The choice of the number of principal components/eigenfaces to keep depends on the analyst. I chose to keep 10 (out of 36)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are some examples of the eigenfaces generated (when reshaped and converted back to images)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/eigfaces1to3.jpg&#34; alt=&#34;first eigenfaces&#34;&gt;&lt;/p&gt;
&lt;p&gt;I know, it&amp;rsquo;s a bit creepy.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Create a classifier. In order to do that, we project all the images in the new low-dimensional space of faces. That basically means obtaining their representation in the new space. We do the same with a new image. We then compare this representation  with each of the projected train images (with the euclidean distance). The decision rule is to match the new presented image with the most similar image from the training set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results were: accuracy of 1 when the task was to match persons (ignoring everything else), accuracy of 0.61 when the correct match of facial expression was evaluated. The results are not that bad, considering the small sample size and that the expected accuracy for the facial expression matching task when choosing at random is 1/6 (approx. 0.16)&lt;/p&gt;
&lt;p&gt;In the report I add some extra information if you want to know more. The code is available &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/eigenfaces&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Brunton, S. L., &amp;amp; Kutz, J. N. (2020). Singular Value Decomposition (SVD). In S.L. Brunton &amp;amp; J.N. Kutz, &lt;em&gt;Data-driven science and engineering: Machine learning, dynamical systems, and control&lt;/em&gt;, (pp. 1-47). Cambridge University Press.&lt;/p&gt;
&lt;p&gt;Ebner, N., Riediger, M., &amp;amp; Lindenberger, U. (2010). FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation. &lt;em&gt;Behavior research Methods&lt;/em&gt;, 42, 351-362. doi:10.3758/BRM.42.1.351.&lt;/p&gt;
&lt;p&gt;Turk, M., &amp;amp; Pentland, A. (1991). Eigenfaces for recognition. &lt;em&gt;Journal of cognitive neuroscience&lt;/em&gt;, 3(1), 71-86.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Distinguishing between real and imagined memories with Bayesian logistic regression</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/bayesianregression/</link>
       <pubDate>Sun, 01 May 2022 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/bayesianregression/</guid>
       <description>&lt;p&gt;This brief research project was part of the course &amp;ldquo;Bayesian data analysis&amp;rdquo; in the Master in Methodology for behavioral sciences.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;In 2020, Sap et al. made an study in which they analyzed the content of real and imagined stories with Natural Language Processing techniques. The paper is quite interesting, but it is also possible to study the difference between the type of event (real/recalled or imagined) from a more conventional analytical approach. Here I propose to study the relationship between type of event and two of the variables collected, distraction during the writing of the story and the importance attributed to the event described.&lt;/p&gt;
&lt;p&gt;A positive relationship is hypothesized between distraction and the probability that the narrated event will be remembered. In principle, creating a story should involve greater cognitive effort. Similarly, among those who write the event while being more distracted, it is expected that there will be a higher proportion of recalled events.
On the other hand, the existence of a positive relationship between importance and the probability that the event is real is suggested. This hypothesis is based on the intuitive idea that it is more reasonable to attribute relevance to events or memories that are real, rather than simply imagined.&lt;/p&gt;
&lt;p&gt;A convenient way to test our hypotheses is by fitting a logistic regression model. However, we are going to doing from a Bayesian perspective. Among other things that means we are going to consider the effects as random variables, not fixed constants. One benefit is that we can build probability (not confidence) intervals around the parameters. We will see later examples of this.&lt;/p&gt;
&lt;p&gt;The complete model specification:&lt;/p&gt;
&lt;p&gt;Each observation $i$ can correspond either to a a real event (${\bf y}_i=1$) or an imagined event (${\bf y}_i=0$) with probabilities $\boldsymbol{\pi}_i$ and $1-\boldsymbol{\pi}_i$ respectively. Note here that ${\bf y}$ and $\boldsymbol{\pi}$ are n-dimensional vectors because we have $n$ observations in the dataset. 
$$
{\bf y}_i \sim Bernoulli(\boldsymbol{\pi}_i)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;
The probability of the type of event can be modeled by the logistic function. The predictors are ${\bf z}_1$ and ${\bf z}_2$, the standardized values of the attributes distraction and importance.&lt;/p&gt;
&lt;p&gt;$$
\boldsymbol{\pi}_i \sim Logistic({\bf z}_1^{(i)},{\bf z}_2^{(i)};\beta_0,\beta_1,\beta_2)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priors&lt;/strong&gt;
The specification of the prior beliefs about the parameters involved in regression is what makes the Bayesian approach different. Here I follow the recommendations of Gelman et al. (2008) to define weak informative prior distributions for the effect/slope of each variable and the intercept. It&amp;rsquo;s important to note that the scale parameter is a standard deviation, not a precision (the reciprocal).&lt;/p&gt;
&lt;p&gt;$$
\beta_0 \sim Cauchy(0,10) 
$$&lt;/p&gt;
&lt;p&gt;$$
\beta_1 \sim Cauchy(0,2.5) 
$$&lt;/p&gt;
&lt;p&gt;$$
\beta_2 \sim Cauchy(0,2.5) 
$$&lt;/p&gt;
&lt;h3 id=&#34;parameter-estimation-simplistic-explanation&#34;&gt;Parameter estimation (simplistic explanation)&lt;/h3&gt;
&lt;p&gt;In essence, Bayesian reasoning is about updating prior beliefs as more data is gathered. In our case, we want to update the different beliefs $p(\beta_j)$ to the posterior beliefs $p(\beta_j|data)$. One way to estimate this posterior distribution is by using Markov Chain Monte Carlo (MCMC) methods.They are approximate only&lt;/p&gt;
&lt;p&gt;To see more about how MCMC was implemented you can check the &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/bayeslogistic.pdf&#34;&gt;report&lt;/a&gt; I made.&lt;/p&gt;
&lt;h3 id=&#34;summary-of-results&#34;&gt;Summary of results&lt;/h3&gt;
&lt;p&gt;The result of the Bayesian inference can be easily visualized. In the next plot we see the case of the distraction effect. The blue curve represents prior &amp;ldquo;allocation of credibility across possibilities&amp;rdquo; (Kruschke, 2015). We chose a weakly informative prior, meaning that we didn&amp;rsquo;t assign much weight to any of the values possible for the effect of the distraction. However, after seeing the data, our belief about that variable changes. The red line represents the posterior density function, after the reallocation of credibility. Most probable values are around 0.2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/pripostbeta1ENG.jpg&#34; alt=&#34;prior vs posterior&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In fact the 95% credible interval (see report) tells us that the effect of distraction lies, with a probability of 0.95, between 0.195 and 0.326, with mean 0.259. And what does mean? It indicates that the actual event odds increase 1.295 points for each extra z-score on the distraction variable, keeping the importance variable constant. For example, knowing that a subject scores very high on that trait (let us assume it is one standard deviation above the mean), we predict an average increase of 30% in the odds that the event described is real with respect to the &amp;ldquo;base&amp;rdquo; odds where we assume a mean value on distraction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With respect to $\beta_2$, it has a much larger average magnitude (1.382), with a 95% credible interval of (1.298, 1.469). Holding distraction constant, the odds of actual event increases by a factor of 3.982 for each standard deviation increase in the importance variable. Knowing that a subject considers the narrated story very important to him/her (scoring one standard deviation above the mean, for example), the probability goes from being approximately 0.5 (having assumed mean score in importance) to 0.79. All this considering the average value of $\beta_2$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of classification performance, the accuracy obtained was 0.73. But that value might not be reliable enough. It was computed using all the data available and taking the mean parameter values. The risk of overfitting is high. Assessing the performance of the model wasn&amp;rsquo;t the main goal though. In any case, this is the plot generated with some of the decision boundaries we would get with different values for the regression coefficients.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/decbound_bayesEN.jpg&#34; alt=&#34;decision boundaries&#34;&gt;&lt;/p&gt;
&lt;p&gt;I know I&amp;rsquo;ve left out important information and results but you can check the complete report &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/bayeslogistic.pdf&#34;&gt;here&lt;/a&gt;. Also the appendix with the code and figures can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/appendixbayes.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Gelman, A., Jakulin, A., Pittau, M. G., &amp;amp; Su, Y. S. (2008). A weakly informative default prior 
distribution for logistic and other regression models. The annals of applied 
statistics, 2(4), 1360-1383.&lt;/p&gt;
&lt;p&gt;Kruschke, J. (2015). Introduction: Credibility, Models,and Parameters. In J. Kruschke, Doing Bayesian data 
analysis: A tutorial with R, JAGS, and Stan (pp. 16-31). Elsevier.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Random walks and decision making</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/randomwalk/</link>
       <pubDate>Fri, 25 Feb 2022 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/randomwalk/</guid>
       <description>&lt;p&gt;What do the motion of particles in a medium (Brownian motion) and decision making have in common? Surprisingly enough, both phenomena can be mathematically modeled in the same way, as stochastic diffusion processes.&lt;/p&gt;
&lt;p&gt;In psychology, the conceptual framework is more or less like this: A binary choice problem is presented to a decision maker, e.g., choose between candidate A and candidate B. We assume that the decision making process involves sequentially examining data samples (Wang &amp;amp; Busemeyer, 2021). We don&amp;rsquo;t know what kind of data is actually using the decision maker, but they serve as evidence favoring one or other alternative. In our voting example, we are assuming that the person is gathering information about the candidates, either explicitly and consciously (assessing the arguments given by each one) or implicitly and in a non-conscious way (being affected by peripheral cues such as attractiveness (see &lt;a href=&#34;https://dictionary.apa.org/peripheral-cue&#34;&gt;https://dictionary.apa.org/peripheral-cue&lt;/a&gt;)). It&amp;rsquo;s important to note that, obviously, we can&amp;rsquo;t observe the cognitive processes happening, so we cannot know what data the agent is assessing nor the value of the increment (incremented preference for A/B). However we can consider it as a process that varies randomly and model it probabilistically.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;The critical idea is that the decision-maker accumulates  these evidence increments across time until the cumulative evidence becomes sufficiently strong (&amp;hellip;)&lt;/em&gt;&amp;rdquo; (Wang &amp;amp; Busemeyer, 2021 p.122-23). Here, sufficiently strong means that the value representing the cumulative evidence reaches a threshold or decision criteria, so that a decision is finally made. After some time $t$ our voter will have enough information favoring A (or B) to choose A (or B).&lt;/p&gt;
&lt;p&gt;State-of-the-art models are rather complex and I just started learning about stochastic processes, but I find this area very interesting, so I decided to develop a very simple model, a random walk model. The algorithm is very basic. At every moment $t$, the agent can increment the &amp;ldquo;valence&amp;rdquo; for the alternative A by 1 point, increment the valence for B by 1 point (decrement the valence for A by -1) or stay the same. What really ends up happening every timestep depends on the probability of each possibility.  At the same time, those probabilities are conditioned by some extra parameters. Specifically, the probability of encountering or sampling a piece of information that favors A depends on a sampling bias. Psychologically speaking, this could be some kind of memory bias. The bigger the bias, the more likely to encounter evidence that increments the valence. However, not every piece of information is accepted, the information can favor A, but we don&amp;rsquo;t necessarily consider it valid. The probability of accepting the evidence and updating the value of the valence is controlled by a acceptance bias. The bigger it is, the easier to accept the information as valid. A low bias might indicate some form of &amp;ldquo;reticence&amp;rdquo; or conservatism. The details are given in the report. But here are some results of simulating different &amp;ldquo;profiles&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/randomwalks.jpg&#34; alt=&#34;random walk examples&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, when there is some preference for one of the alternatives, less time is needed to make a decision. Also, when the decision threshold is set significantly higher (in absolute value) comparing to other people, the response time increases.&lt;/p&gt;
&lt;p&gt;From this random walk model we can make some interesting predictions. After some calculations we can obtain a formula that can let us predict the mean time needed to make a choice under some fixed conditions. If you want to know more, you can check the report.&lt;/p&gt;
&lt;p&gt;Finally, one of the things I found most exciting about these stochastic models, is that you can simulate something like the &amp;ldquo;development of the preference&amp;rdquo;. Imagine our decision maker has the decision boundary for A at 10, and the decision boundary at -10. That means that in order to choose A she needs accumulate evidence with a valence of 10. The same goes for B. In our model, that means there are 20+1 possible states or values of valence (counting 0). If we treat our random walk process as a Markov chain we can achieve some interesting predictions like knowing the probability of being in each state (valence) at any time. In the gif below we can see how this probabilities change as time goes on. Because the person simulated has some bias towards A, the probability distribution slowly shifts to the right, where valence for A is higher. By time $t=100$, the valence of 10 has the highest probability, indicating a clear preference and that the person have probably chosen A by that that time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/animation.gif&#34; alt=&#34;markov chain evolution&#34;&gt;&lt;/p&gt;
&lt;p&gt;Full details of the algorithm, some of the mathematical details of the Markov chains and the R code can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/randomwalks_report.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>My first internship: data mining in a real world setting</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/firstdataexperience/</link>
       <pubDate>Mon, 20 Dec 2021 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/firstdataexperience/</guid>
       <description>&lt;p&gt;Eventhough I&amp;rsquo;ve played with real data sets in other occasions, I had never had the chance to work on a real project. But in September 2021, things changed, I started working as an intern at a engineering company called GRUPO STIN. My role wasn&amp;rsquo;t extremely well defined, most of the time I was doing data analysis, but I also designed and run field experiments, which sounds more data scientist-ish to me. The project in which I was involved is related with smartwatch data, but I cannot say much about it due to the non-disclosure agreement I signed. However, that doesn&amp;rsquo;t really matter. What I would like to share are some of the things I&amp;rsquo;ve learnt or that I consider to be more relevant about the experience.&lt;/p&gt;
&lt;h3 id=&#34;the-importance-of-data-preprocessing&#34;&gt;The importance of data preprocessing&lt;/h3&gt;
&lt;p&gt;This is something I already knew, the fact that data cleaning, data integration&amp;hellip; can easily occupy half or even more of the time spent on the project. I can now say that&amp;rsquo;s not an overestimation. When the data you&amp;rsquo;re going to use comes from different sources and is gathered from far from perfectly reliable devices (in this case smartwatches) in far from perfect conditions you end up devoting quite a lot of time doing all kind of data preprocessing. In my case I had to deal with very noisy data, with a lot of outliers and &amp;ldquo;strange&amp;rdquo; values. Also, data integration was crucial, I needed to combine different types of data coming in different file formats all the time. But the point is that all the time spent is worth it. I think data preprocessing is a step you&amp;rsquo;ll always revisit when you receive new data, but once you have identified and dealt with the main issues the first time, the process becomes more automated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advice to myself (or anyone interested)&lt;/strong&gt;: Spend some time with data cleaning and examine your data carefully before doing any kind of predictive or inferential analysis.&lt;/p&gt;
&lt;h3 id=&#34;the-importance-of-data-exploration&#34;&gt;The importance of data exploration&lt;/h3&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is also one of those steps in which is worth investing a decent amount of time. That was especially true in my case, because I was working in a novel project and little was known about what to expect from the data. &amp;ldquo;Fortunately&amp;rdquo; for me, I&amp;rsquo;m still a rookie in the machine learning area, so I wasn&amp;rsquo;t specially eager to try some fancy algorithm I knew so I didn&amp;rsquo;t overlook EDA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advice to myself (or anyone interested)&lt;/strong&gt;: Don&amp;rsquo;t forget doing EDA, it&amp;rsquo;s sometimes surprising how much information you can get from some boxplots and descriptive analysis.&lt;/p&gt;
&lt;h3 id=&#34;the-importance-of-visualization&#34;&gt;The importance of visualization&lt;/h3&gt;
&lt;p&gt;Again, a topic mentioned many times, but with a reason. I think visualizations and plots are some of the most powerful ways to convey complex information both to experts and non-experts. And I would say more, based on my experience. Sometimes it is the only or most useful way to extract some knowledge from data. In my case, for example, we were interested in analyzing an specific pattern in data, but there were not (to my knowledge) any appropiate statistical technique for data. However, there was a way to put the data into a plot to do some &amp;ldquo;visual analytics&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advice to myself (or anyone interested)&lt;/strong&gt;: It can be hard with multidimensional data, but always try to make some plots and display the information graphically. They can make you notice things that &amp;ldquo;plain numbers&amp;rdquo; and tables can&amp;rsquo;t, and they make the communication of results easier.&lt;/p&gt;
&lt;h3 id=&#34;the-importance-of-versatility&#34;&gt;The importance of versatility&lt;/h3&gt;
&lt;p&gt;Most tasks in a data science workflow, from data cleaning, to data transformation and statistical analysis, can be done entirely with one tool, like Python (assuming you don&amp;rsquo;t need to manage Big Data and SQL databases, like my case). Nonetheless, it can be very beneficial to know other programming languages and software tools, because some tasks are better suited for some languages. For example, R works great if you need inferential statistics and linear models. You can use Python too, of course, but R is just better and offers more options (my opinion). Sometimes you can even use SPSS or JASP if you need to get some p-values quick. That was precisely my case, almost all the time I was using python, but I switched to R or even other statistical packages when I thought I needed it. And I believe I worked faster that way, because I didn&amp;rsquo;t need to learn how to do X in python when I already knew how to do it in R, or viceversa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advice to myself (or anyone interested)&lt;/strong&gt;: I don&amp;rsquo;t think is bad to stick with a programming language, but be flexible and learn others, it can become very handy.&lt;/p&gt;
&lt;h3 id=&#34;the-importance-of-organization&#34;&gt;The importance of organization&lt;/h3&gt;
&lt;p&gt;That is something I admit I could have done better. I worked mainly with Jupyter lab, and because I was always exploring new data, testing some interesting functions, making prototypes&amp;hellip; I ended up generating a lot of files and folders. I managed to find everything I needed when I needed, but it was a bit of a chaos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advice to myself (or anyone interested)&lt;/strong&gt;: If you are responsible of storing the data and files generated by your analysis, organize the folder dedicated to the project. Don&amp;rsquo;t improvise and create folders and folders just because you don&amp;rsquo;t know where to save your data/notebooks/scripts (like I did). Remove everything you don&amp;rsquo;t need.&lt;/p&gt;
&lt;h3 id=&#34;extra-the-importance-of-efficiency-and-scalability&#34;&gt;(extra) The importance of efficiency and scalability&lt;/h3&gt;
&lt;p&gt;The project in which I participated was a Proof Of Concept (POC), like a pilot study, so I didn&amp;rsquo;t really work with huge data sets, but still I got a taste of the challenges and problems Big Data can generate. One time I had to discard the idea of using an algorithm I was thinking of using because when I tested with a sample of data I ran into memory issues. In that moment I realized that sooner or later I would need to use cloud services to be able to move to the &amp;ldquo;next level&amp;rdquo;
.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advice to myself (or anyone interested)&lt;/strong&gt;: Learn cloud computing, you&amp;rsquo;ll need it.&lt;/p&gt;
&lt;p&gt;Hope you&amp;rsquo;ll find it useful!&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Titanic: Machine Learning from Disaster</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/kaggletitanic/</link>
       <pubDate>Mon, 06 Sep 2021 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/kaggletitanic/</guid>
       <description>&lt;p&gt;Famous Kaggle project for beginners in machine learning.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Eventhough nearly a year has passed since I started studying seriously data analysis and two since I learned Python, I&amp;rsquo;ve never tried doing the famous kaggle project of Titanic. I can still be considered beginner so this project suits me.&lt;/p&gt;
&lt;p&gt;There is a good description of the competition (data sets used, attributes&amp;hellip;) in the &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;web&lt;/a&gt;, here I&amp;rsquo;ll write briefly about my approach to the project.&lt;/p&gt;
&lt;h3 id=&#34;workflow&#34;&gt;Workflow&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/workflow.jpg&#34; alt=&#34;steps&#34;&gt;&lt;/p&gt;
&lt;p&gt;I tried to follow the typical machine learning workflow but given that my main objective was to practice with the python library sklearn I focused more on the model selection phase. I was aware that doing more feature engineering would have improved my final model, but I was fine with that. In my future projects I&amp;rsquo;ll spend more time on that part of the process.&lt;/p&gt;
&lt;p&gt;Most of the algorithm building is data-driven, meaning that the choice of the imputing method, scaling method&amp;hellip; was made based on cross-validation scores. I ended up with three algorithms that performed similar: logistic regression model, K-nearest neighbors classifier and decision tree classifier. I &amp;ldquo;combined&amp;rdquo; them using a simple voting classifier to form the final model.&lt;/p&gt;
&lt;p&gt;The accuracy obtainded in the test set was approximately 0.78&lt;/p&gt;
&lt;p&gt;The jupyter notebook with more detailed description of the steps can be found &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/titanic&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>LU decomposition from scratch</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/ludecomp/</link>
       <pubDate>Sat, 24 Jul 2021 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/ludecomp/</guid>
       <description>&lt;p&gt;Implementation of the LU factorization in Python.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Linear systems of the form ${\bf Ax}={\bf b}$ are essential in Linear Algebra and they appear in multiple applied contexts. When ${\bf A}$ is an $n \times n$ matrix, we know the $n \times 1$ solution vector ${\bf x}$ is unique, but that doesn&amp;rsquo;t mean it&amp;rsquo;s easy to find. We can use Gaussian elimination or find the inverse matrix ${\bf A}^{-1}$, but those methods are not feasible when $n$ is large.&lt;/p&gt;
&lt;p&gt;A more efficient strategy (commonly used by computer software) consists of decomposing ${\bf A}$ into the product of a lower triangular matrix ${\bf L}$ and an upper triangular matrix ${\bf U}$. This new system $({\bf LU}){\bf x}={\bf b}$ is much easier to solve, eventhough it requires to first solve ${\bf L}{\bf y}={\bf b}$ and then ${\bf U}{\bf x}={\bf y}$ (See &lt;a href=&#34;https://www.amazon.com/Linear-Algebra-Step-Kuldeep-Singh/dp/0199654441&#34;&gt;Singh (2013)&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Following the theory explained in Singh (2013), I decided to implement the method from scratch. The complete code can be found &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/LU_decomposition/tree/main&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Singh, K. (2013).  Linear algebra: step by step. Oxford University Press.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Simple convolutional neural network for digit recognition</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/firstconvnnet/</link>
       <pubDate>Sun, 09 May 2021 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/firstconvnnet/</guid>
       <description>&lt;p&gt;Assignment for &amp;ldquo;Knowledge technology&amp;rdquo;, course in the Master in Methodology for behavioral sciences.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;The purpose of the the project wasn&amp;rsquo;t to develop a sophisticated architecture (see the image below to get a sense of what kind of network we built) or to get highest accuracy in a test set. Rather we had  to show that we had understood the fundamental concepts of convolutional networks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/netarch.jpg&#34; alt=&#34;Simplified network architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;The most interesting thing about this project was the fact that I had to build my own digit data set and then use it as test set. If you want to know more about it you can check the &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_cnn.pdf&#34;&gt;report&lt;/a&gt; I made. The supplementary material can be found on my &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/First_Convolutional_Net&#34;&gt;github profile&lt;/a&gt;. I also made a few ppt slides to visually represent the net architecture. You can view the pdf &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/arquitectura_red.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Helping behavior during COVID-19 lockdown</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/helpbehav/</link>
       <pubDate>Thu, 29 Apr 2021 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/helpbehav/</guid>
       <description>&lt;p&gt;This brief research project was part of the course &amp;ldquo;Linear models&amp;rdquo; in the Master in Methodology for behavioral sciences.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Why we help other people is a question psychologists have been trying to answer for decades. Researchers usually design experiments in which participants have the opportunity to help. However, a non-experimental approach can also shed some light on the issue.&lt;/p&gt;
&lt;p&gt;Here I adopt that perspective and make use of a &lt;a href=&#34;http://www.cis.es/cis/opencm/ES/2_bancodatos/estudios/ver.jsp?estudio=14530&#34;&gt;public survey&lt;/a&gt; conducted by the spanish Center for Sociological Research (CIS) after the end of the 2020 lockdown. Because the outcome of interest, helping behavior, is described in the survey as a dichotomic variable (either you help or you don&amp;rsquo;t), I decide to fit a logistic model of the form:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat P(help_i=\text{Yes}|{\bf x}_i)=\frac{1}{1+e^{-(\beta_0+{\boldsymbol{\beta}^\intercal\cdot {\bf x}_i})}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where ${\bf x}_i$ represents the vector of $J$ features or explanatory variables of the $i$-th subject. Read the equation as: the estimated probability of helping for person $i$, given a set of features that person has, is modeled as a (logistic) function of a linear combination of those features.&lt;/p&gt;
&lt;p&gt;The model specification is partially based on Schwartz and Howard (1984). They describe five stages in helping behavior:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;attention&lt;/strong&gt; to need,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generation of value-based &lt;strong&gt;motivation&lt;/strong&gt;,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;awareness&lt;/strong&gt; of potential actions,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;evaluation&lt;/strong&gt; of the costs and benefits of potential actions, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;overt behavior or inaction&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, I don&amp;rsquo;t contemplate values as the main motives for helping behavior but empathy. The features finally selected were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Type of people affected (attention),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Purposes of change (motivation),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feeling of being helpful (awareness/self-efficacy),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grief over loss of loved one (empathy),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Concern over loss of job (empathy) and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sex.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full description of  them can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/helping_behavior_reglog.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;summary-of-results&#34;&gt;Summary of results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Cross-validated Nagelkerke&amp;rsquo;s $R^2$ is approximately 0.07. The proposed model reduces the misfit of the null model by 7%.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the point of view of the model&amp;rsquo;s classification performance, the results are also modest, with an AUC value of 0.62.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The profile of people with the lowest propensity to help is as follows: men who consider that the crisis has mainly affected the people who have suffered it directly, have not suffered from the loss of family, friends or employment, have not made any intention to change, and do not consider their actions to be particularly helpful. The prognosis is an odds ratio of 0.468, which implies that the probability of helping is 53.2% lower than the probability of not helping.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The indirect measures of empathy are the best predictors of helping behavior. Holding all else constant, the odds of helping is 1.715 times greater among individuals who report having suffered from the loss of a family member or friend than among those who do not report having suffered this experience. This data is consistent with that found in the sample, in which the proportion (in percentage) of people who say they help and have suffered from the loss of people is 15 points higher than that of people who say they help but have not suffered from the loss of loved ones.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The complete report can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/helping_behavior_reglog.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Simulation of QUARTO! board game and minimax algorithm</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/minimax/</link>
       <pubDate>Sun, 11 Apr 2021 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/minimax/</guid>
       <description>&lt;p&gt;Final project of the &amp;ldquo;Simulation techniques&amp;rdquo; subject in the Master in Methodology for behavioral sciences. For this assignment we were given a short description of the QUARTO! board game rules (slightly modified) and a clear objective: to simulate it in R and to design a game strategy. The performance of the game strategy had to be better than a strategy  based on random selection of moves.&lt;/p&gt;
&lt;h3 id=&#34;game-description&#34;&gt;Game description&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/quartoboardNpieces.jpg&#34; alt=&#34;QUARTO board game and pieces&#34;&gt;&lt;/p&gt;
&lt;p&gt;The rules of this strategy game are easy: &amp;ldquo;The wooden pieces have four characteristics: light/dark, tall/short, square/round, or hollow/filled. Each turn, an opponent chooses which piece you play; if you form a line of four pieces with one characteristic in common, you win. If you can’t, you choose a piece for your opponent to play. The game progresses until a winner is decided or all pieces have been played.&amp;rdquo; (&lt;a href=&#34;https://www.hammacher.com/product/award-winning-quarto-board-game&#34;&gt;Source:www.hammacher.com&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;However, in our case the rules were slightly modified. The player doesn&amp;rsquo;t choose the opponent&amp;rsquo;s pieces, she chooses her own pieces. That change makes the game more similar to tic-tac-toe or connect 4.&lt;/p&gt;
&lt;h3 id=&#34;quarto-in-code&#34;&gt;QUARTO! in code&lt;/h3&gt;
&lt;p&gt;How can the game be represented in R?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pieces: There are $2^4=16$ different pieces that result from the combination of different properties (2 different properties for each of the 4 attributes). Because the specific property or attribute the piece has doesn&amp;rsquo;t make any difference to the game, I decided to represent the set of all pieces as a 16 x 4 numeric matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# expand.grid does the same but it&amp;#39;s a bit slower&lt;/span&gt;
create.pieces &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(){
   &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,each&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,times&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),
                &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,each&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,times&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
                &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,each&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,times&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;),
                &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,each&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,times&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;)))
}

&lt;span style=&#34;color:#a6e22e&#34;&gt;create.pieces&lt;/span&gt;()

      [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
 [1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
 [2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
 [3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
 [4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
 [5,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
 [6,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
 [7,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
 [8,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
 [9,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
[10,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
[11,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
[12,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
[13,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
[14,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
[15,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;
[16,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Board: The board must be able to store information about position, but also about the attributes of every piece, because they are all different. For that reason I created a 3D board, an array of dimensions 4 x 4 x 4:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;board &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;array&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;))

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Moving a piece to a certain position in the board is now as easy as doing this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;fourth_piece &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create.pieces&lt;/span&gt;()[4,] &lt;span style=&#34;color:#75715e&#34;&gt;# [1 3 6 8]&lt;/span&gt;
board[2,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; fourth_piece

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

, , &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;

     [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
[1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
[4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To avoid mistakes and using the same piece or position twice, we need to keep track of the available pieces and positions. We need an object for both. We saw the pieces can be stored in a matrix, and that&amp;rsquo;s also possible with the available moves.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;pieces &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create.pieces&lt;/span&gt;()
( positions &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,each&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)) )

      [,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] [,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
 [1,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
 [2,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
 [3,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
 [4,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
 [5,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
 [6,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
 [7,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
 [8,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
 [9,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
[10,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
[11,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
[12,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
[13,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
[14,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
[15,]    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A complete move by one player could be like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;board &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;array&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;))
positions &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,each&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)) 
pieces &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create.pieces&lt;/span&gt;()

&lt;span style=&#34;color:#75715e&#34;&gt;# choose piece and position&lt;/span&gt;
pos &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; positions[5,] &lt;span style=&#34;color:#75715e&#34;&gt;# 1 x 2 vector &lt;/span&gt;
piece &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; pieces[4,] &lt;span style=&#34;color:#75715e&#34;&gt;# 1 x 4 vector &lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# move piece to position&lt;/span&gt;
board[pos[1],pos[2],] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; piece

&lt;span style=&#34;color:#75715e&#34;&gt;# update remaining pieces and positions&lt;/span&gt;
positions &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; positions[&lt;span style=&#34;color:#ae81ff&#34;&gt;-5&lt;/span&gt;,] &lt;span style=&#34;color:#75715e&#34;&gt;# 15 x 2 matrix&lt;/span&gt;
pieces &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; pieces[&lt;span style=&#34;color:#ae81ff&#34;&gt;-4&lt;/span&gt;,] &lt;span style=&#34;color:#75715e&#34;&gt;# 15 x 2 matrix &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we are capable a simulating a complete game (link to code at the end), but the way players choose their pieces and moves is totally random at this moment.&lt;/p&gt;
&lt;h3 id=&#34;game-strategy-minimax-and-alpha-beta&#34;&gt;Game strategy: Minimax and alpha-beta&lt;/h3&gt;
&lt;p&gt;In order to create an artificial agent that plays rationally I decided to implement the minimax algorithm with alpha-beta pruning. Technically they are different algorithms, but I consider the latter as an improvement or extension of the former.&lt;/p&gt;
&lt;p&gt;And how do they work? Good explanations can be found in Russell &amp;amp; Norvig (2020) and Wikipedia:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Minimax&lt;/strong&gt;: &amp;ldquo;Given a game tree, the optimal strategy can be determined from the minimax value of each state in the tree (&amp;hellip;). The minimax value is the utility (for player A, for example) of being in the corresponding state, assuming that both players play optimally from there to the end of the game.&amp;rdquo; The utility given by the payoff function &amp;ldquo;indicates how good it would be for a player to reach that position&amp;rdquo;.Obviously, &amp;ldquo;The minimax value of a terminal state is just its utility&amp;rdquo; (score associated with win, loss, draw or other complex states). Minimax works by choosing the next move based on these utilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, for instance, &amp;ldquo;If player A can win in one move, his best move is that winning move. If player B knows that one move will lead to the situation where player A can win in one move, while another move will lead to the situation where player A can, at best, draw, then player B&amp;rsquo;s best move is the one leading to a draw. Late in the game, it&amp;rsquo;s easy to see what the &amp;ldquo;best&amp;rdquo; move is. The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B&amp;rsquo;s own chances of winning).&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Alpha-beta pruning&lt;/strong&gt;: Minimax is a recursive algorithm that works by searching or exploring the states of the game tree. That becomes a problem even with moderately complex games because &amp;ldquo;the number of game states is exponential in the depth of the tree&amp;rdquo;. That means that &amp;ldquo;raw&amp;rdquo; or naïve minimax is often impractical. However, we can use the technique of alpha-beta pruning to prune (leave unexplored) &amp;ldquo;large parts of the tree that make no difference to the outcome&amp;rdquo;. It&amp;rsquo;s basically the same as minimax but &amp;ldquo;It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;rsquo;ve fundamentally based the implementation of the algorithm on the pseudocode provided by Russell &amp;amp; Norvig (2020). However, it&amp;rsquo;s not exactly the same. I reproduce here the pseudocode that best represents the program I designed, and again, the link to the actual code can be found at the end of the post.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;FUNCTION &lt;span style=&#34;color:#a6e22e&#34;&gt;AB_MINIMAX&lt;/span&gt;(board,ply,player,positions,pieces,depth&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;infinity,beta&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;infinity)

   best &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(value,pos,piece)
   
   terminal_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_terminal&lt;/span&gt;(board,ply&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;depth)
   
   IF terminal_state OR depth &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; THEN
      best.value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;payoff&lt;/span&gt;(terminal_state,player,depth)
      RETURN best
   ELSE
      IF player &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; THEN
         v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;infinity
         FOR each position in positions
            FOR each piece in pieces
               updated &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;result&lt;/span&gt;(board,positions,position,pieces,piece)
               output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;AB_MINIMAX&lt;/span&gt;(updated.board,ply,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;player,
                                   updated.positions,updated.pieces,
                                   depth&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;,alpha,beta)
                        
               IF output.value &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; v THEN
                  v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.value
                  best.value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.value
                  best.pos &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; position
                  best.piece &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; piece
                  alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(alpha,v)
               ENDIF
               
               IF v &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; beta THEN
                  RETURN best
               ENDIF
                  
            ENDFOR
         ENDFOR
         
      ELSE
         v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; infinity
         FOR each position in positions
            FOR each piece in pieces
               updated &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;result&lt;/span&gt;(board,positions,position,pieces,piece)
               output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;AB_MINIMAX&lt;/span&gt;(updated.board,ply,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;player,
                        updated.positions,updated.pieces,
                        depth&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;,alpha,beta)
                        
               IF output.value &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; v THEN
                  v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.value
                  best.value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.value
                  best.pos &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; position
                  best.piece &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; piece
                  alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(alpha,v)
               ENDIF
               
               IF v &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; alpha THEN
                  RETURN best
               ENDIF
                  
            ENDFOR
         ENDFOR
         
      ENDIF
      
   ENDIF
   
   ENDFUNCTION
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;the-issue-of-efficiency&#34;&gt;The issue of efficiency&lt;/h3&gt;
&lt;p&gt;I originally tried to make everything in R, but I quickly noticed that the program was terribly slow. To reduce the running time I decided to take advantage of the package &lt;code&gt;rcpp&lt;/code&gt; and I re-wrote almost everything in C++ code. The improvement was remarkable. Here you can see the time needed to make a move after 12 &amp;ldquo;plies&amp;rdquo; or turns:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;microbenchmark&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;microbenchmark&lt;/span&gt;(
   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;R&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#a6e22e&#34;&gt;AB_MINIMAX_R&lt;/span&gt;(demo&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;board,ply&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;,player&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,demo&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;positions_left,demo&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;pieces_left)},
   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;C++&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#a6e22e&#34;&gt;AB_MINIMAX&lt;/span&gt;(demo&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;board,ply&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;,player&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,demo&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;positions_left,demo&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;pieces_left)}
   )
   
Unit&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; microseconds
expr    min      lq      mean      median     uq      max      neval     cld
   R   &lt;span style=&#34;color:#ae81ff&#34;&gt;13242.1&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;14604.3&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;20137.87&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;16809.8&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;22100.9&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;57384.4&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;        b
   C&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;259.1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;296.1&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;375.22&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;322.5&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;407.2&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;1144.8&lt;/span&gt;     &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;        a
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/QUARTO-and-minimax&#34;&gt;Link&lt;/a&gt; to code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_quarto.pdf&#34;&gt;Link&lt;/a&gt; to the report.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Human vs. &amp;ldquo;machine&amp;rdquo;: Eventhough I didn&amp;rsquo;t mention it, the game has been designed so that it can be played by a real human. I&amp;rsquo;ve made a second version of the main program with an user interface that has been modified to be more &amp;ldquo;attractive&amp;rdquo; and less confusing (converting the 3D board to a bidimensional matrix, for example).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Russell, S. &amp;amp; Norvig, P. (2020). Adversarial Search and Games. In S. Russell y P. Norvig, &lt;em&gt;Artificial
Intelligence: a modern approach&lt;/em&gt; (4a ed) pp.(146-180). Pearson.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Predicting stroke (III): Clustering</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/strokecluster/</link>
       <pubDate>Thu, 11 Mar 2021 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/strokecluster/</guid>
       <description>&lt;p&gt;Last part of the series &lt;em&gt;Predicting stroke&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;So far, all the analyses that have been performed were aimed at solving a supervised learning problem, namely, classifying individuals who suffer, or not, stroke. At all times it is known to which category each one belongs, and therefore it is possible to assess the accuracy in the classification.&lt;/p&gt;
&lt;p&gt;However, it is also possible to pose another type of problem. Without taking into consideration the class to which each subject belongs, can we form from the predictor variables two groups that correspond to the profile of individuals who suffer stroke and the profile of those who do not suffer stroke?&lt;/p&gt;
&lt;p&gt;That kind of question can be answered using clustering techniques such us K-means&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;After performing a k-means analysis assuming $k=2$ we get the following results&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/overviewkmeans.PNG&#34; alt=&#34;Rapidminer output&#34;&gt;&lt;/p&gt;
&lt;p&gt;The characteristics of the of the clusters generated are pretty similar to what we could expect theoretically. The vast majority of subjects belong to cluster 0, which we can call low-risk, and a smaller percentage is categorized as high-risk. It can be observed in the characteristics of cluster 1 that both age and glucose level are more than 40% higher, which is consistent with what is known (Boehme et al, 2017). In addition, among these subjects, hypertension is clearly higher, although since it is a categorical variable, the interpretation is more useful in terms of proportions.&lt;/p&gt;
&lt;p&gt;If you want to know more, check the full &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_nosuperv_Diego_Henandez.pdf&#34;&gt;report&lt;/a&gt;. The Rapidminer file cab found clicking on this &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Predicting-stroke/tree/main/KMEANS&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Boehme, A. K., Esenwa, C., y Elkind, M. S. (2017). Stroke risk factors, genetics, and prevention. &lt;em&gt;Circulation
research&lt;/em&gt;, 120(3), 472-495.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Predicting stroke (II): Model selection</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/strokeeval/</link>
       <pubDate>Wed, 10 Mar 2021 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/strokeeval/</guid>
       <description>&lt;p&gt;Second part of the series &lt;em&gt;Predicting stroke&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;So far we&amp;rsquo;ve seen different models and approaches to the problem of classifying people in the &amp;ldquo;stroke&amp;rdquo; group and the &amp;ldquo;no-stroke&amp;rdquo; group. However, we haven&amp;rsquo;t compared them yet. In order to make a good decision about which model to choose as best, we must compare them under the most similar conditions. That&amp;rsquo;s the purpose of this project. The models are fitted and then different performance metrics are computed.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;$F_1$&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;AUC&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Sensitivity&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Specificity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;QDA&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,6331&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,0803&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,8396&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0,8876&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,6284&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Decision tree&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0,7482&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0,1050&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,8118&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,8186&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0,7469&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;SVM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,7218&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,0974&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,8507&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,8314&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0,7198&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(In bold type the highest score obtained for each statistic)&lt;/p&gt;
&lt;p&gt;A graphical depiction of the ROC curves is also very useful&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/rocsmodelos.png&#34; alt=&#34;ROC curves&#34;&gt;&lt;/p&gt;
&lt;p&gt;The best model in terms of AUC, a commonly used performance measure, is the SVM. Despite that, for various reasons I finally decided to choose the decision tree model as the best one.&lt;/p&gt;
&lt;p&gt;Report can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/evalmodels.pdf&#34;&gt;here&lt;/a&gt; and the two-page &amp;ldquo;executive report&amp;rdquo; &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/executive_report.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;link to &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Predicting-stroke/blob/main/stroke2.csv&#34;&gt;data set&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;That was my first time seriously doing model comparison and model selection so I probably made a lot of mistakes. I know, for instance, that the comparison is not completely legit because the models are not totally equivalent (i.e. different number of predictors and different ones).&lt;/li&gt;
&lt;/ul&gt;
</description>
     </item>
   
     <item>
       <title>Predicting stroke (I): Trying different classification algorithms</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/strokemodels/</link>
       <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/strokemodels/</guid>
       <description>&lt;p&gt;Assignment for &amp;ldquo;Knowledge technology&amp;rdquo;, course in the Master in Methodology for behavioral sciences. This post correspond to the first part of the &amp;ldquo;series&amp;rdquo; &lt;em&gt;Predicting stroke&lt;/em&gt;, which consists of a set of independent works in which I try to solve an applied machine learning problem: to accurately classify people who have suffered a stroke or not, based on different predictors.&lt;/p&gt;
&lt;h3 id=&#34;description-of-data&#34;&gt;Description of data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Predicting-stroke/blob/main/stroke2.csv&#34;&gt;&lt;em&gt;stroke2.csv&lt;/em&gt;&lt;/a&gt;.  The data used to be in Kaggle but I had to download it from a github repository of someone who had analyzed the data set before. Here you can see the data before the preprocessing.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;gender&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;hypertension&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;heart_disease&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;ever_married&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;work_type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;children&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Private&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Private&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;hellip; (43000 x 10)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Residence_type&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;avg_glucose_level&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;bmi&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;smoking_status&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;stroke&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rural&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;95.12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;never smoked&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87.96&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;never smoked&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;110.89&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;17.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;never smoked&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;hellip; (43000 x 10)&lt;/p&gt;
&lt;p&gt;The variables are self-explanatory but I had no codebook so whenever I encountered inconsistencies or &amp;ldquo;strange&amp;rdquo; things I had to interpret the data. For example, the third observation corresponds to a kid of 8 years old who works in the private sector. Something must be wrong but is it the age or the work type? It&amp;rsquo;s more plausible that work type is wrong, but technically we don&amp;rsquo;t know what is wrongly coded.&lt;/p&gt;
&lt;p&gt;One important thing to note about the data set is the frequency distribution of the outcome variable. 98% of the sample is people who haven&amp;rsquo;t suffered a stroke. This severe imbalance can be very troublesome (&lt;a href=&#34;https://en.wikipedia.org/wiki/Accuracy_paradox&#34;&gt;accuracy &amp;ldquo;paradox&amp;rdquo;&lt;/a&gt;: you get 98% overall accuracy but with 0% sensitivity because all instances are classified as members of the majority class) so it&amp;rsquo;s neccesary to deal with it.&lt;/p&gt;
&lt;p&gt;For the first model I opted for the downsampling strategy. If the minority class has $n$ observations, you select a sample of $n$ from the set of instances that belong to the majority class. Now you have a new sample of $n+n$ examples and no assymmetry in the variable of interest. The problem is that you reduce the total sample size, you loose information and that you are changing the natural distribution of the variable just for convenience (the prevalence of stroke is not 50%). For those reasons I changed the strategy. In the decision tree and SVM models I used cost-sensitive learning, which involves imposing a cost matrix that weights false negatives as more serious errors. Intuitively we could say that you need to be more confident in order to classify someone as belonging to the stroke category (more details in the &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/evalmodels.pdf&#34;&gt;model evaluation report&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;classification-models&#34;&gt;Classification models&lt;/h3&gt;
&lt;h4 id=&#34;linear-discriminant-analysis&#34;&gt;Linear Discriminant Analysis&lt;/h4&gt;
&lt;p&gt;Final model (quadratic discriminant function really, not linear) includes only age as predictor&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;qda=fitcdiscr(featQDA,group, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;DiscrimType&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;quadratic&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;CrossVal&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;on&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cost&amp;#39;&lt;/span&gt;,[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;&lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]);

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Predicting-stroke/tree/main/LDA&#34;&gt;Link&lt;/a&gt; to code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_LDA_Diego_Hernandez.pdf&#34;&gt;Link&lt;/a&gt; to the report.&lt;/p&gt;
&lt;h4 id=&#34;decision-tree&#34;&gt;Decision tree&lt;/h4&gt;
&lt;p&gt;Final model includes age, bmi (both discretized), hypertension and heart disease as predictors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;tree=fitctree(feattree,group, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;CategoricalPredictors&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;all&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SplitCriterion&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;deviance&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;MinLeafSize&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;MinParentSize&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;230&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;CrossVal&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;on&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;... &lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cost&amp;#39;&lt;/span&gt;,[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;&lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Predicting-stroke/tree/main/DT&#34;&gt;Link&lt;/a&gt; to code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_arboles_Diego_Hernandez.pdf&#34;&gt;Link&lt;/a&gt; to the report.&lt;/p&gt;
&lt;h4 id=&#34;support-vector-machine&#34;&gt;Support Vector Machine&lt;/h4&gt;
&lt;p&gt;Final model includes age, avg_glucose_level, hypertension, smoking_status and heart_disease as features.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;SVM=fitcsvm(featSVM,group, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;CategoricalPredictors&amp;#39;&lt;/span&gt;,{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;hypertension&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;smoking_status&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;heart_disease&amp;#39;&lt;/span&gt;}, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BoxConstraint&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.24&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;KernelFunction&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;CrossVal&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;on&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cost&amp;#39;&lt;/span&gt;,[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;&lt;span style=&#34;color:#ae81ff&#34;&gt;55&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Predicting-stroke/tree/main/SVM&#34;&gt;Link&lt;/a&gt; to code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_SVM_Diego_Henandez.pdf&#34;&gt;Link&lt;/a&gt; to the report.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This was my first time applying Linear Discriminant Analysis, decision trees, SVMs and also the first time using Matlab. I&amp;rsquo;ve experimented a lot doing this project and I&amp;rsquo;ve probably committed a lot of mistakes. The hyperparemeter tuning process was a bit like a mess and I don&amp;rsquo;t expect them to be optimal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The class imbalance problem gave me a lot of headaches. I tried different approaches to overcome the problem (downsampling and cost-sensitive learning), and I finally stuck with one approach but I&amp;rsquo;m not sure that was the best decision.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
     </item>
   
     <item>
       <title>ANOVA from scratch</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/anova/</link>
       <pubDate>Sat, 26 Dec 2020 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/anova/</guid>
       <description>&lt;p&gt;Project developed for the &amp;ldquo;Computer methods&amp;rdquo; course in the Master in Methodology for behavioral sciences. Full description and examples can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/aov.pdf&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/ANOVA_scratch&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;main-function&#34;&gt;Main function&lt;/h3&gt;
&lt;p&gt;ANOVA: Perform a one- or two-way fixed-effects ANOVA.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;factor&lt;/strong&gt;: character vector of length 1 or 2 with the name of the predictors or factors. They must refer to a variable of type factor of a dataframe. In case the factor(s) are not of type factor, the function &lt;strong&gt;check_factor&lt;/strong&gt; is called to transform the dataframe variables into factor type.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;vd&lt;/strong&gt;: character vector of length 1 with the name of the outcome or dependent variable. It must refer to a numeric variable of a dataframe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;data&lt;/strong&gt;: dataframe containing the factor or factors and the dependent variable.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;output:&lt;/p&gt;
&lt;p&gt;Depending on the number of factors, returns the function &lt;strong&gt;one_way&lt;/strong&gt; or the function &lt;strong&gt;two_way&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;ANOVA &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(factor,vd,data){
  
  num_fact &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(factor)
  
  data &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;check_factor&lt;/span&gt;(factor,data)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;switch&lt;/span&gt;(num_fact,
         
         &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;one_way&lt;/span&gt;(num_fact,factor,vd,data)),
         &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;two_way&lt;/span&gt;(num_fact,factor,vd,data)))
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;auxiliary-functions&#34;&gt;Auxiliary functions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;check_factor&lt;/strong&gt;: Checks that the predictors are type factor. If they are not, it transforms the variables into factors in the original dataframe itself and returns it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
check_factor &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(vars,df){
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(var in vars){
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;is.factor&lt;/span&gt;(df[,var])))
      
      &lt;span style=&#34;color:#a6e22e&#34;&gt;warning&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;se convertira &amp;#39;&lt;/span&gt;,var,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; en tipo factor &amp;#39;&lt;/span&gt;))
      df[,var] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as.factor&lt;/span&gt;(df[,var])
  }
  &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(df)
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;anova_plot&lt;/strong&gt;: Returns line graph with means for each level (one-way ANOVA) or for each combination of levels (two-way ANOVA). Allows exploration of the interaction effect by visual inspection.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;anova_plot &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(num_fact,factor,vd,data,medias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,J&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;){
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if&lt;/span&gt;(num_fact&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;){
    
    labels &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;levels&lt;/span&gt;(data[,factor])

    &lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;J,medias,
         type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;o&amp;#39;&lt;/span&gt;, lwd&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,
         ylim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;min&lt;/span&gt;(data[,vd]),&lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(data[,vd])),
         xlab&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;factor,ylab&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;vd,
         pch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;19&lt;/span&gt;, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#ff6b6b&amp;#39;&lt;/span&gt;,
         xaxt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;n&amp;#39;&lt;/span&gt;)
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;axis&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;J,labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;labels) &lt;span style=&#34;color:#75715e&#34;&gt;# xticks&lt;/span&gt;
  }
  
  else{
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;interaction.plot&lt;/span&gt;(x.factor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[,factor[1]], trace.factor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[,factor[2]], 
                 response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[,vd], fun &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mean, 
                 type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;, legend &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;, lwd&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,
                 ylim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;min&lt;/span&gt;(data[,vd]),&lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(data[,vd])),
                 xlab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; factor[1], ylab&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;vd,trace.label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,
                 pch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;19&lt;/span&gt;), col &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#4ecdc4&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#ff6b6b&amp;#39;&lt;/span&gt;))
  }
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;grid&lt;/span&gt;(col &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lightgray&amp;#34;&lt;/span&gt;, lty &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dotted&amp;#34;&lt;/span&gt;,
         lwd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;par&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lwd&amp;#34;&lt;/span&gt;), equilogs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# rejilla para ver mejor&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;one_way&lt;/strong&gt; : Perform one-way fixed-effects ANOVA. The procedure is based on the chapter on one-way ANOVA in Pardo and San Martín (2015). The function provides a list with three dataframes containing the value of the F statistic, its critical level and three measures of effect size.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
one_way &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(num_fact,factor,vd,data){
  
  N &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(data)                                   &lt;span style=&#34;color:#75715e&#34;&gt;# Tamaño muestral&lt;/span&gt;
  J &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nlevels&lt;/span&gt;(data[,factor])                       &lt;span style=&#34;color:#75715e&#34;&gt;# Número de niveles del factor A&lt;/span&gt;
  
 &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Estimador variabilidad intra basado &lt;/span&gt;
 &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   en varianzas de cada nivel j del factor&lt;/span&gt;

  varianzas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                         by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor]]),
                         FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;var)[[vd]]
  
  n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                 by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor]]),
                 FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;length)[[vd]]
  
  SCE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;((n&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;varianzas)
  glE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; N&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;J
  MCE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCE&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glE
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Estimador variabilidad inter&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   basado en las medias del factor&lt;/span&gt;

  medias_A_df &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                           by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor]]),
                           FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mean)
  medias_A &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; medias_A_df[[vd]]             &lt;span style=&#34;color:#75715e&#34;&gt;# el df es para la salida&lt;/span&gt;
  
  media_tot &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mean&lt;/span&gt;(data[,vd])
  SCI &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;( n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (medias_A&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;media_tot)^2  )
  glI &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; J&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;
  MCI &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCI&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glI
  
  F_stat &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; MCI&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;MCE
  
  p_val &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pf&lt;/span&gt;(F_stat,glI,glE)
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Salida&lt;/span&gt;
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;anova_plot&lt;/span&gt;(num_fact,factor,vd,data,medias_A,J)

 &lt;span style=&#34;color:#75715e&#34;&gt;# -------------------------------------------------&lt;/span&gt;
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;names&lt;/span&gt;(medias_A_df) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(factor,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;medias&amp;#39;&lt;/span&gt;)
  
  sumas_cuad &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCT&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCI&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;SCE,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCI&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCI,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCE&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCE)
  contraste &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Factores&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;factor,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F_stat,
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nivel crítico&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;p_val)

  &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;medias&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;medias_A_df,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sumas cuadráticas&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sumas_cuad,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;contraste&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;contraste,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tamaño efecto&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;effect_size&lt;/span&gt;(num_fact,N,
                                          glE,
                                          glA&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;glI,F_A&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F_stat)))
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;two_way&lt;/strong&gt; : Performs two-way ANOVA with fixed effects and assuming the same number of participants per combination of factor levels (balanced design). The procedure is based on the chapter on two-factor ANOVA in Pardo and San Martín (2015). The function provides a list with three dataframes containing the value of the F-statistic for each factor and for the interaction, the associated critical levels and three measures of effect size for each.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
two_way &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(num_fact,factor,vd,data){
  
  N &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(data)                                 &lt;span style=&#34;color:#75715e&#34;&gt;# Tamaño muestral&lt;/span&gt;
  J &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nlevels&lt;/span&gt;(data[,factor[1]])                  &lt;span style=&#34;color:#75715e&#34;&gt;# Número de niveles del factor A&lt;/span&gt;
  K &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nlevels&lt;/span&gt;(data[,factor[2]])                  &lt;span style=&#34;color:#75715e&#34;&gt;# Número de niveles del factor B&lt;/span&gt;
  n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; N&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(J&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;K)                                    &lt;span style=&#34;color:#75715e&#34;&gt;# Número de sujetos por combinación jk&lt;/span&gt;
                                                  &lt;span style=&#34;color:#75715e&#34;&gt;# (se asume anova con diseño equilibrado)&lt;/span&gt;
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# -------------------------------------------------&lt;/span&gt;
  
  media_tot &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mean&lt;/span&gt;(data[,vd])
  
  SCT &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;((data[vd]&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;media_tot)^2)
  glT &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; N&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;
 &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Estimador variabilidad intra &lt;/span&gt;
 &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   basado en varianzas de cada combinación jk&lt;/span&gt;

  varianzas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                         by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor[1]]],data[[factor[2]]]),
                         FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;var)[[vd]]
  SCE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;((n&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;varianzas)
  glE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; N&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(J&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;K)
  MCE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCE&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glE
  
 &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Estimador variabilidad inter&lt;/span&gt;
 &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   basado en medias del factor A&lt;/span&gt;
  medias_A &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                        by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor[1]]]),
                        FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mean)[[vd]]
  
  SCa &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; n&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;K&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;( (medias_A&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;media_tot)^2 )
  glA &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; J&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;
  MCa &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCa&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glA
  F_stat_A &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; MCa&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;MCE
  
  p_val_A &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pf&lt;/span&gt;(F_stat_A,glA,glE)
  
 &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Estimador variabilidad inter&lt;/span&gt;
 &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   basado en medias del factor B&lt;/span&gt;
         
  medias_B &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                        by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor[2]]]),
                        FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mean)[[vd]]
  
  SCb &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; n&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;J&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;( (medias_B&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;media_tot)^2 )
  glB &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; K&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;
  MCb &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCb&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glB
  F_stat_B &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; MCb&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;MCE
  
  p_val_B &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pf&lt;/span&gt;(F_stat_B,glB,glE)
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Estimador variabilidad inter&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   basado en medias del factor interacción&lt;/span&gt;

  medias_inter_df &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(data[vd],
                        by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data[[factor[1]]],data[[factor[2]]]),
                        FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mean)
  medias_inter &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; medias_inter_df[[vd]]               &lt;span style=&#34;color:#75715e&#34;&gt;# el df es para la salida&lt;/span&gt;
  
  SCab &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCT&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(SCa&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;SCb&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;SCE)
  glAB &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; (J&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(K&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;)
  MCab &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCab&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glAB
  F_stat_AB &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; MCab&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;MCE
  
  p_val_AB &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pf&lt;/span&gt;(F_stat_AB,glAB,glE)

  &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Ajuste condicional. Compara&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#                                                   modelo aditivo e interactivo&lt;/span&gt;
  
  aditiv_vs_inter &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ajuste_condicional&lt;/span&gt;(factor,vd,
                                        SCT,SCa,SCb,SCE,
                                        glT,glA,glB,glE)
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# ------------------------------------------------- Salida&lt;/span&gt;
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;anova_plot&lt;/span&gt;(num_fact,factor,vd,data)
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# -------------------------------------------------&lt;/span&gt;


  &lt;span style=&#34;color:#a6e22e&#34;&gt;names&lt;/span&gt;(medias_inter_df) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(factor,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;medias&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# nombrar adecuadamente df con medias&lt;/span&gt;
  
  sumas_cuad &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCT&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCT,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCa&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCa,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCb&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCb,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCab&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCab,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SCE&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SCE)
  contraste &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Factores&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(factor,&lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(factor[1],&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;,factor[2])),
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(F_stat_A,F_stat_B,F_stat_AB),
                          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nivel crítico&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(p_val_A,p_val_B,p_val_AB))
  
  tamano_efecto &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(contraste[1],
                                       &lt;span style=&#34;color:#a6e22e&#34;&gt;effect_size&lt;/span&gt;(num_fact,N,
                                                   glE,
                                                   glA,F_A&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F_stat_A,
                                                   glB,F_B&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F_stat_B,
                                                   glAB,F_AB&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F_stat_AB))
  &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ajuste_condicional&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;aditiv_vs_inter,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;medias&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;medias_inter_df,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sumas cuadráticas&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sumas_cuad,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;contraste&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;contraste,
              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tamaño efecto&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tamano_efecto))
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;ajuste_condicional&lt;/strong&gt;: It compares the fit of the simple model that includes only main effects (additive model) versus that of the more complex model, which includes the interaction component (interactive model). A non-statistically significant result indicates that there is no evidence that the interactive model produces a significant reduction in prediction errors compared to the simpler model. In this case a warning is given to the user, indicating that the ANOVA model generated may not be optimal. The procedure has been copied from Ato and Vallejo (2015) (p.138).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
ajuste_condicional &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(factor,vd,
                               SCT,SCa,SCb,SCE,
                               glT,glA,glB,glE){
  
  SCE_aditivo &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCT&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(SCa&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;SCb)
  glE_aditivo &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; glT&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(glA&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;glB)
  
  SCE_inter &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; SCE            &lt;span style=&#34;color:#75715e&#34;&gt;# SCE de modelo inter = SCT-(SCa+SCb+SCab)&lt;/span&gt;
  glE_inter &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; glE            &lt;span style=&#34;color:#75715e&#34;&gt;# glE de modelo inter = glT-(glA+glB+glAB)&lt;/span&gt;
  
  F_stat &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; ( (SCE_aditivo&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;SCE_inter)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(glE_aditivo&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;glE_inter) )&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(SCE_inter&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;glE_inter)
  p_val &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pf&lt;/span&gt;(F_stat,(glE_aditivo&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;glE_inter),(glE_inter))
  
 
 adit &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(vd,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;,factor[1],&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+&amp;#39;&lt;/span&gt;,factor[2])
 inter &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(vd,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;,factor[1],&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+&amp;#39;&lt;/span&gt;,factor[2],&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+&amp;#39;&lt;/span&gt;,factor[1],&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;,factor[2])
 
 comparacion &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;modelo aditivo&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;adit,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;modelo interactivo&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;inter,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F_stat,
                           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nivel crítico&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;p_val)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(p_val&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.05&lt;/span&gt;) &lt;span style=&#34;color:#a6e22e&#34;&gt;warning&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Diferencia no significativa en grado de ajuste con 
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;                          alfa=0,05. El modelo interactivo podría no ser apropiado&amp;#39;&lt;/span&gt;)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;modelos&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(adit,inter),
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;F&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;,F_stat),
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nivel crítico&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;,p_val)))
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;effect_size&lt;/strong&gt; : Calculates three measures of effect size for all factors involved in the ANOVA. Depending on the type of ANOVA (with 1 or 2 factors), it returns some formulas or others. All the formulas used have been copied directly from the chapters dedicated to ANOVA in Pardo and San Martín (2015). Factors are assumed to be fixed effects.
Problem: when comparing results with those obtained with JASP software, it is observed that $\omega^2$ of the function is higher, the discrepancy being quite large in the case of two-factor ANOVA.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;effect_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(num_fact,N,
                        glE,
                        glA,F_A,
                        glB&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,F_B&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,
                        glAB&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,F_AB&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;){
  etas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;()
  omegas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;()
  deltas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;()
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(gl_F in &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(glA,F_A), B&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(glB,F_B), AB&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(glAB,F_AB))){
    
      eta &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; (gl_F[1]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;gl_F[2])&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(gl_F[1]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;gl_F[2]&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;glE)
      omega &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; (gl_F[1]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(gl_F[2]&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(gl_F[1]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(gl_F[2]&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;N)
      delta &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;( omega&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;omega) )
      etas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(etas,eta); omegas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(omegas,omega); deltas &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(deltas,delta)
  }
  
  efs &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;delta Cohen&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; deltas,
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;eta cuadrado&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; etas,
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;omega cuadrado&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; omegas)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(efs)
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;an-example-two-way-anova&#34;&gt;An example: two-way ANOVA&lt;/h1&gt;
&lt;p&gt;This data set, &amp;ldquo;Heart Rate&amp;rdquo;, (example JASP file) provides heart rates of male and female runners and generally sedentary participants following 6 minutes exercise (800 observations). Does gender affect average heart rate? Does lifestyle (runners and non-runners) have an effect on heart rate? Is there an interactive effect of lifestyle and gender on heart rate?&lt;/p&gt;
&lt;p&gt;Variables:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gender&lt;/strong&gt; - Participant&amp;rsquo;s gender (Female, Male).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Group&lt;/strong&gt; - Group of &amp;lsquo;Runners&amp;rsquo; (averaging more than15 miles per week) and &amp;lsquo;Control&amp;rsquo; group (generally sedentary participant).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Heart.Rate&lt;/strong&gt; - Heart rate after six minutes of exercise.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/5q42c/&#34;&gt;link to data&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Gender&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Group&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Heart.Rate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Runners&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;119&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Runners&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Runners&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Runners&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;119&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;ANOVA&lt;/span&gt;(factor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Group&amp;#39;&lt;/span&gt;),vd&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Heart.Rate&amp;#39;&lt;/span&gt;,data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/aovplot.png&#34; alt=&#34;anova interaction plot&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ajuste_condicional
                               modelos        F nivel.crítico
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;              Heart.Rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Gender&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;Group       &lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; Heart.Rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Gender&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;Group&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;Gender&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;Group &lt;span style=&#34;color:#ae81ff&#34;&gt;7.409481&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.006629953&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;medias
  Gender   Group  medias
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; Female Control &lt;span style=&#34;color:#ae81ff&#34;&gt;148.000&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;   Male Control &lt;span style=&#34;color:#ae81ff&#34;&gt;130.000&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; Female Runners &lt;span style=&#34;color:#ae81ff&#34;&gt;115.985&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;   Male Runners &lt;span style=&#34;color:#ae81ff&#34;&gt;103.975&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sumas_cuadráticas
       SCT      SCa      SCb     SCab      SCE
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;407985.9&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;45030.01&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;168432.1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1794.005&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;192729.8&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;contraste
      Factores          F nivel.crítico
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;       Gender &lt;span style=&#34;color:#ae81ff&#34;&gt;185.979949&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.000000000&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;        Group &lt;span style=&#34;color:#ae81ff&#34;&gt;695.647040&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.000000000&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; Gender&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;Group   &lt;span style=&#34;color:#ae81ff&#34;&gt;7.409481&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.006629953&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;tamaño_efecto
      Factores delta.Cohen eta.cuadrado omega.cuadrado
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;       Gender  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.48085854&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.189392817&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0.187800726&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;        Group  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.93183089&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.466361694&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0.464756575&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; Gender&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;Group  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.08950894&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.009222546&lt;/span&gt;    &lt;span style=&#34;color:#ae81ff&#34;&gt;0.007948171&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;References&lt;/p&gt;
&lt;p&gt;Pardo, A., &amp;amp; San Martín, R. (2015). Análisis de datos en ciencias sociales y de salud II. Editorial Síntesis.&lt;/p&gt;
&lt;p&gt;Ato, M. &amp;amp; Vallejo, G. (2015). Diseños de investigación en Psicología. Ediciones Pirámide.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Definite integral</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/integral/</link>
       <pubDate>Fri, 06 Nov 2020 00:00:00 +0100</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/integral/</guid>
       <description>&lt;p&gt;This small program consists of a function that can compute the definite integral of a function over a given interval. Full description with examples can be found &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/integral.pdf&#34;&gt;here&lt;/a&gt; and the code is &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Integral-definida&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;strong&gt;integral_definida&lt;/strong&gt;: approximates the definite integral of a function over [a,b], given a number of partitions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;funcion&lt;/strong&gt;: Mathematical function that we want to integrate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;a&lt;/strong&gt;: Lower limit of integration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;b&lt;/strong&gt;: Upper limit of integration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;n&lt;/strong&gt;: Number of partitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;aprox&lt;/strong&gt;: Chosen integration technique . The definite integral can be approximated by rectangles (&lt;strong&gt;&amp;lsquo;rect&amp;rsquo;&lt;/strong&gt;), the trapezoid rule (&lt;strong&gt;&amp;lsquo;trap&amp;rsquo;&lt;/strong&gt;) or by Simpson&amp;rsquo;s rule (&lt;strong&gt;&amp;lsquo;simp&amp;rsquo;&lt;/strong&gt;). The latter gives the best approximations, but when n is an even number.&lt;/p&gt;
&lt;p&gt;Approximation by rectangles: $\small\displaystyle \int_a^b  f(x)dx = \lim_{n\rightarrow \infty}  I_n=\lim_{n\rightarrow \infty}S_n$.&lt;/p&gt;
&lt;p&gt;$\small In=\sum_{i=1}^n f(m_i)\cdot (\frac{b-a}{n})$ ; $\small Sn=\sum_{i=1}^n f(M_i)\cdot (\frac{b-a}{n})$&lt;/p&gt;
&lt;p&gt;Trapezoid rule:  $\small\displaystyle \int_a^b f(x) dx \approx \frac{b-a}{2n}[f(x_0) + 2f(x_1) + 2f(x_2) + 2f(x_3) + &amp;hellip; + 2f(x_{n-2}) + 2f(x_{n-1}) + f(x_n)]$&lt;/p&gt;
&lt;p&gt;Simpson&amp;rsquo;s rule:  $\small\displaystyle \int_a^b f(x) dx \approx \frac{b-a}{3n}[f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + &amp;hellip; + 2f(x_{n-2}) + 4f(x_{n-1}) + f(x_n)]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If &lt;strong&gt;aprox=&#39;rect&amp;rsquo;&lt;/strong&gt;: list with two elements representing the lower sum (In) and the upper sum (Sn).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If &lt;strong&gt;aprox=&#39;trap&amp;rsquo;&lt;/strong&gt; o &lt;strong&gt;aprox=&#39;simp&amp;rsquo;&lt;/strong&gt;: vector of length one representing the estimate of the definite integral over a and b.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
integral_definida &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(funcion,a,b,n,aprox&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rect&amp;#39;&lt;/span&gt;){

  interv &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; (b&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;a)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;n
  
  seg &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;seq&lt;/span&gt;(from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;a,to&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;b,by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;interv)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(aprox&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;trap&amp;#39;&lt;/span&gt;){
    
    strt_end &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[1]),&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[n&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;]))
    mid &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[2&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n])
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(( (b&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;a)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n) )&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(strt_end,mid))
  }
  else &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(aprox&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;simp&amp;#39;&lt;/span&gt;){
    
    strt_end &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[1]),&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[n&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;]))
    seg_cut &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; seg[2&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n]
    
    indices &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(seg_cut)
    mid &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg_cut[indices&lt;span style=&#34;color:#f92672&#34;&gt;%%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]),
             &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg_cut[indices&lt;span style=&#34;color:#f92672&#34;&gt;%%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(( (b&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;a)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n) )&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(strt_end,mid))
  }
  else{
    
    In &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;double&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    Sn &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;double&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
  
    &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n){
      fmi &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;min&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[i]&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;seg[i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;]))
      In &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; In&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;fmi&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;interv
      fMi &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;funcion&lt;/span&gt;(seg[i]&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;seg[i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;]))
      Sn &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; Sn&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;fMi&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;interv
    }
    &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(In&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;In,Sn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Sn))
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
     </item>
   
     <item>
       <title>Is it normal?</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/isnormal/</link>
       <pubDate>Tue, 20 Oct 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/isnormal/</guid>
       <description>&lt;p&gt;Function to support the Kolmogorov-Smirnov test (for normality) graphically.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;A common task for many scientists, regarding data analysis, and especially in behavioral sciences, is to test if some variable of interest is normally distributed. One of the statistical techniques that is usually used is the Kolmogorov-Smirnov test (KS).It is based on the comparison of two distribution functions, one empirical ($F(Y_i)$), estimated from the collected data, and one theoretical ($F(Y_0)$). With KS we can test the null hypothesis that the empirical distribution is equal to the theoretical distribution, in this case, the normal distribution, with the parameters that we specify.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_0:F(Y_i)=F(Y_0)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H_1:F(Y_i)\neq F(Y_0)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Eventhough this technique is very useful, it can be misleading sometimes because with big samples, even small deviations from normality are deemed statistically significant. To avoid dichotomic thinking (significant=&#39;bad&amp;rsquo; and non-significant=&#39;good&amp;rsquo;), we can complement this test with some plots. Because KS is based on the comparison of two distribution functions, we can literally show this comparison by plotting the theoretical cumulative distribution function and the empirical distribution function. The goodness of fit can be assessed by looking if the two curves plotted overlap or not.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ks_test&lt;/span&gt;(Yi,mu,sigma):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Takes an array of values and returns its CDF and
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    its theoretical CDF if it were normal. Also returns
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    the value of the DKS statistic.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Parameters
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    ----------
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Yi : 1-D array-like (np.array or list)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        Vector of values that correspond to the observed values
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        of our variable of interest.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    mu : int or float
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        Expected value of the normal distribution we are testing.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    sigma : int or float
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        Standard deviation of the normal distribution we are testing.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Returns
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    -------
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    float.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        Value corresponding to the statistic DKS:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            DKS=max(abs(empirical_CDF - theoretical_CDF))
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    
    sorted_Yi&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(Yi)
    emp_cdf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(start&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,stop&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len(Yi)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(len(Yi))
    theor_cdf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;norm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cdf(sorted_Yi,loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mu,scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sigma)
    DKS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;max(abs(emp_cdf&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;theor_cdf))
    
    fig,ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots()

    fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;suptitle(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Kolmogorov-Smirnov visual test&amp;#39;&lt;/span&gt;, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;, fontweight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bold&amp;#39;&lt;/span&gt;)

    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(sorted_Yi,emp_cdf,
               label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Empirical CDF&amp;#39;&lt;/span&gt;,
               c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#ff6b6b&amp;#39;&lt;/span&gt;,
               edgecolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;black&amp;#39;&lt;/span&gt;,
               linewidths&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,
              alpha&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(sorted_Yi,emp_cdf,c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#ff6b6b&amp;#39;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# just to see the &amp;#39;trajectory of the points&amp;#39;&lt;/span&gt;

    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(sorted_Yi,theor_cdf,
               label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Theoretical CDF&amp;#39;&lt;/span&gt;,
               c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#4ecdc4&amp;#39;&lt;/span&gt;,
               edgecolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;black&amp;#39;&lt;/span&gt;,
               linewidths&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,
              alpha&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(sorted_Yi,theor_cdf,c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#4ecdc4&amp;#39;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# just to see the &amp;#39;trajectory of the points&amp;#39;&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(Yi,pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Series):
        ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xlabel(Yi&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;$Y_i$&amp;#39;&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cumulative probability&amp;#39;&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend(loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;center right&amp;#39;&lt;/span&gt;);
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; DKS

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;some-examples&#34;&gt;Some examples&lt;/h3&gt;
&lt;p&gt;Imagine we want to know if newborns from mother who smoke tend to weigth less. Now imagine we can only collect data from 14 babies. It&amp;rsquo;s a very small sample, so we can&amp;rsquo;t rely on the Central Limit Theorem. Before trying to compare means or something like that, it would be necessary to check if we can assume that variable &amp;lsquo;weigth of newborns&amp;rsquo; is normally distributed. We can quickly do that with the KS &amp;lsquo;visual&amp;rsquo; test.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weigths corresponding to the 14 babies&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
Yi&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1.48&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.93&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.98&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.04&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.08&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.18&lt;/span&gt;,
            &lt;span style=&#34;color:#ae81ff&#34;&gt;2.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.45&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.47&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3.15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3.46&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.64&lt;/span&gt;]) &lt;span style=&#34;color:#75715e&#34;&gt;# data from Pardo &amp;amp; Ruiz (2015) pp.68&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Is the data normally distributed with mean equal to 2.5 and standard deviation equal to 1?&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;ks_test(Yi,mu&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2.5&lt;/span&gt;,sigma&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/ksexample.png&#34; alt=&#34;fancy plot of CDFs&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can see that the data is clearly not normal.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at another example. This one is made to show what happens when the variable is normal. We have a sample of 100 observations corresponding to scores in some recently developed intelligence test.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IQ scores of 100 people&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;IQ&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal(loc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;,size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;),
                columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;IQ Scores&amp;#39;&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Are the scores of our test normally distributed with mean equal to 100 and standard deviation equal to 15?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/ksexampleci.png&#34; alt=&#34;fancy plot of CDFs&#34;&gt;&lt;/p&gt;
&lt;p&gt;There is a great overlap, so we can conclude that the variable follows a normal distribution with $\mu$=100 and $\sigma$=15.&lt;/p&gt;
&lt;p&gt;If you want to see the complete code, you can visit my &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Kolmogorov-Smirnov-visual-test&#34;&gt;Github&lt;/a&gt; profile.&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Who is the next Google&#39;s UX engineer?</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/googleux/</link>
       <pubDate>Wed, 09 Sep 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/googleux/</guid>
       <description>&lt;p&gt;A group of colleagues and I made a hypothetical selection profile for the position
of UX engineer at Google. We also assessed a fictional candidate based on that profile.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/UXpic.PNG&#34; alt=&#34;Ux wireframes&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;This project is part of the module Psychological Assessment, which I studied
in the third year of Psychology.&lt;/p&gt;
&lt;p&gt;The objective was to design a selection profile for a job from a psychometric perspective,
or trait perspective. That means we only focused on personality or intelligence attributes.
We also set some decision criteria based on minimum required scores, weight or relative importance of each attribute&amp;hellip;&lt;/p&gt;
&lt;p&gt;Finally, one person was taken as a candidate for the position and was actually evaluated on the relevant traits. A decision was made based on the decision criteria that we established in the profile.&lt;/p&gt;
&lt;p&gt;Click &lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/informe_ux.pdf&#34;&gt;here&lt;/a&gt; if you want to know more about the UX engineer profile
we made (in Spanish).&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Monty Hall problem simulation</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/monty_hall/</link>
       <pubDate>Tue, 08 Sep 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/monty_hall/</guid>
       <description>&lt;p&gt;I&amp;rsquo;ve written a simple program that simulates the famous problem. To do that, I&amp;rsquo;ve taken
advantage of some of the object-oriented features in Python.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you&amp;rsquo;re on a game show, and you&amp;rsquo;re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what&amp;rsquo;s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, &lt;strong&gt;&amp;ldquo;Do you want to pick door No. 2?&amp;rdquo; Is it to your advantage to switch your choice?&lt;/strong&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Monty_Hall_problem&#34;&gt;(&lt;em&gt;Source:Wikipedia&lt;/em&gt;)&lt;/a&gt;
&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/monty.png&#34; alt=&#34;What should you do?&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is often said that it doesn&amp;rsquo;t really matter what decision is made, because, after all, both doors have the same probability of hiding the prize, which is 1/3. However, intuition is playing tricks on us in this case, because there is indeed an optimal decision. The probabilities aren&amp;rsquo;t the same. This can be demonstrated, but, better yet, it can be shown empirically.&lt;/p&gt;
&lt;p&gt;How? Because probability is considered the &amp;ldquo;long term&amp;rdquo; frequency of the occurrence of an event (from the frequentist view), we can simulate the game thousands of times and record the results that produce different decisions. The frequency of wins and loses when switching the choice or staying with the first choice will be
good approximations of the real probabilities.&lt;/p&gt;
&lt;p&gt;Go and check the complete code on my &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Monty-Hall-problem-&#34;&gt;Github&lt;/a&gt; if you want to know the answer!&lt;/p&gt;
</description>
     </item>
   
     <item>
       <title>Newton method from scratch</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/newtonmet/</link>
       <pubDate>Tue, 01 Sep 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/newtonmet/</guid>
       <description>&lt;p&gt;Attemp to implement the Newton-Raphson method in Python&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Studying calculus I found this interesting numeric method for approximating the real zeros of a function.
That is, for estimating solutions for equations of the form $f(x)=0$. The algorithm can be described as follows &lt;a href=&#34;https://www.amazon.com/-/es/Ron-Larson/dp/1285057090&#34;&gt;(Larson &amp;amp; Edwards, 2013)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;Let $f(c)=0$. where $f$ is differentiable on an open interval containing $c$. Then, to approximate $c$, use these steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make an initial estimate $x_1$ that is close to $c$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Determine a new approximation $x_{n+1}=x_n-\frac{f(x_n)}{f&amp;rsquo;(x_n)}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When $|x_n-x_{n+1}|$ is within the desired accuracy, let $x_{n+1}$ serve as the final approximation. Otherwise, return to Step 2 and calculate a new approximation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, this method doesn&amp;rsquo;t always yield a convergent sequence. This is an important issue. It means that there are functions whose zeros can&amp;rsquo;t be approximated using this technique. If we don&amp;rsquo;t detect those cases, we will end up applying the Newton method indefinitley without ever reaching a solution. Luckily, it can be shown that a condition sufficient to produce convergence to a zero of $f$ is that $\left|\frac{f(x)f&amp;rsquo;&#39;(x)}{[f&amp;rsquo;(x)]^2}\right|&amp;lt;1$ on an open interval containing the zero.&lt;/p&gt;
&lt;p&gt;Now that we have all the necessary &amp;lsquo;ingredients&amp;rsquo; for our function, let&amp;rsquo;s see how it could be implemented.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Iterative formula:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$x_{n+1}=x_n-\frac{f(x_n)}{f&amp;rsquo;(x_n)}$&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;iterative_formula&lt;/span&gt;(f,x_n):
    
    &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.misc &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; derivative
    
    x_n1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;x_n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;(f(x_n)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;derivative(f,x_n))

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x_n1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Convergence condition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\left|\frac{f(x)f&amp;rsquo;&#39;(x)}{[f&amp;rsquo;(x)]^2}\right|&amp;lt;1$&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;converges&lt;/span&gt;(f,x_n):
    
    &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.misc &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; derivative
    
    numerator&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;f(x_n)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;derivative(f,x_n,n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# parameter &amp;#39;n&amp;#39; for order of derivative&lt;/span&gt;
    denominator&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;derivative(f,x_n)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; abs(numerator&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;denominator)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Precision:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$|x_n-x_{n+1}|$&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;precise_enough&lt;/span&gt;(x_n,x_n1,error):
      
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; abs(x_n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x_n1)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;error
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Complete algorithm:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;newton_method&lt;/span&gt;(f,x_n,error&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;):
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; converges(f,x_n):
        
        x_n1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;iterative_formula(f,x_n)

        iters&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; precise_enough(x_n,x_n1,error):
            x_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;x_n1
            x_n1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;iterative_formula(f,x_n)
            iters&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x_n1,iters
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Newton&amp;#39;s method fails to converge&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The code is available &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/Newton-Raphson-method&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I tested the code with several examples (you can check it in my github) and it seems to work fine. However, there are some examples that produce strange results. In other cases, the answer is correct but slightly different from the
obtained using other code versions of the newton method algorithm. I wouldn&amp;rsquo;t say that makes the program a failure, but it necessarily means that I&amp;rsquo;ve written a function that doesn&amp;rsquo;t capture all the elements needed to solve the problem properly.&lt;/li&gt;
&lt;li&gt;The function doesn&amp;rsquo;t return a tuple or a list with all the real zeros, just one solution, because the output depends on the first estimate given as argument. For instance, if you pass the function $f(x)=x^2-2$ and the estimate $x_1=1$, you get as a result $\approx{1.41}$ ($+\sqrt{2}$ ), but if you pass as first approximation $x_1=-1$ you get the other zero, which is $\approx{-1.41}$ ($-\sqrt{2}$ ). This forces the user to have in advance a rough idea of the value of the solutions.&lt;/li&gt;
&lt;/ul&gt;
</description>
     </item>
   
     <item>
       <title>Analyzing Data with Python (edX project)</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/ibm/</link>
       <pubDate>Sat, 20 Jun 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/ibm/</guid>
       <description>&lt;p&gt;Final assigment of the course &lt;strong&gt;IBM: DA0101EN Analyzing Data with Python.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;The goal was to analyze and predict alcohol consumption by country using features such as beer servings and wine servings.&lt;/p&gt;
&lt;p&gt;We were given one data set&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/python-for-data-analysis-project-edx-/blob/master/world_drink_habits.csv&#34;&gt;&lt;em&gt;world_drink_habits.csv&lt;/em&gt;&lt;/a&gt;. All the data analysis was done with this data&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;country&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;beer_servings&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;spirit_servings&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;wine_servings&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;total_litres_of_pure_alcohol&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;continent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Asia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Albania&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;132&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Europe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Algeria&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Africa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip; (193x6)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The specific tasks involved a bit of pre-processing, descriptive analysis and fitting and
evaluating various regression models.&lt;/p&gt;
&lt;p&gt;The complete code is available &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/python-for-data-analysis-project-edx-&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We were given hints and instructions in many steps, so it wasn&amp;rsquo;t so difficult to complete. However, this project
served very well as practice and to develop the data analyst mindset (how to structure the process of analysis)&lt;/li&gt;
&lt;/ul&gt;
</description>
     </item>
   
     <item>
       <title>Using Python for research (edX project)</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/edx/</link>
       <pubDate>Mon, 01 Jun 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/edx/</guid>
       <description>&lt;p&gt;Final assigment of the course &lt;strong&gt;HarvardX: PH526x Using Python for Research.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;The goal was to develop a model that would predict the type of physical activity
(standing, walking, climbing stairs or going down the stairs) from tri-axial smartphone accelerometer data.&lt;/p&gt;
&lt;p&gt;We were given three files of data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/python-for-research-project-edx-/blob/master/train_time_series.csv&#34;&gt;&lt;em&gt;train_time_series.csv&lt;/em&gt;&lt;/a&gt;. Raw accelerometer data. Used to build the model.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;timestamp&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;UTC time&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;x&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;y&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20586&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565109930787&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:45:30.787&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.0064849&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.9348602&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.0690460&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20587&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565109930887&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:45:30.887&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.0664672&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-1.0154418&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.08955383&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20588&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565109930987&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:45:30.987&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.0434875&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-1.0212554&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.17846679&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip; (3744x6)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/python-for-research-project-edx-/blob/master/train_labels.csv&#34;&gt;&lt;em&gt;train_labels.csv&lt;/em&gt;&lt;/a&gt;. The activity labels for every tenth observation in &lt;em&gt;train_time_series.csv&lt;/em&gt;. Also needed to build the model.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;timestamp&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;UTC time&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20589&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565109931087&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:45:31.087&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20599&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565109932090&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:45:32.090&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20609&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565109933092&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:45:33.092&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip; (375x3)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/python-for-research-project-edx-/blob/master/test_time_series.csv&#34;&gt;&lt;em&gt;test_time_series.csv&lt;/em&gt;&lt;/a&gt;. We were asked to provide the activity labels predicted by our code for this test data set.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;timestamp&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;UTC time&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;x&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;y&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;24330&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565110306139&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:51:46.139&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.0342864&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-1.5044555&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.1576232&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;24331&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565110306239&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:51:46.239&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.4091644&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-1.0385437&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.0309753&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;24332&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1565110306340&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2019-08-06T16:51:46.340&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.234390&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.9845581&amp;hellip;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.1247711&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip; (1250x6)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because it was a classification problem I tried several approaches like Knn and
logistic regression. At the end of the course we were not given the data set with the
actual labels but we were told our accuracy score. I achieved a very modest 46%.&lt;/p&gt;
&lt;p&gt;The complete code is available &lt;a href=&#34;https://github.com/Diego-Hernandez-Jimenez/python-for-research-project-edx-&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This was my first data science project, so my knowledge of Python (and data science) was extremely basic. I would
definitely change some things now that I know more.&lt;/li&gt;
&lt;/ul&gt;
</description>
     </item>
   
     <item>
       <title>Smartphone addiction and impulsivity</title>
       <link>https://diego-hernandez-jimenez.github.io/web/posts/tfg/</link>
       <pubDate>Sat, 30 May 2020 00:00:00 +0200</pubDate>
       
       <guid>https://diego-hernandez-jimenez.github.io/web/posts/tfg/</guid>
       <description>&lt;p&gt;Bachelor&amp;rsquo;s thesis/final project (in Spanish, &amp;ldquo;Trabajo de
Fin de Grado&amp;rdquo;). I review past research on the relationship between impulsivity and smartphone
consumption. It&amp;rsquo;s my intention to conduct the study soon, at least with simulated data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://diego-hernandez-jimenez.github.io/web/images/smart_addic.jpg&#34; alt=&#34;chained to our devices&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;In an era of increasing interconnectivity and technological development, smartphones have become a constant in our lives, to the point of even generating dependence problems in some people. But what factors explain this abusive use?&lt;/p&gt;
&lt;p&gt;I focus in this paper mainly on two concepts coming from behavioral economics, &lt;em&gt;delay discounting&lt;/em&gt; and level of &lt;em&gt;demand&lt;/em&gt;. These concepts are closely related to impulsivity and motivation and in the scientific literature it is common to see them associated with problems of substance addiction and gambling. However, their relationship with other addictive-type problems is not yet well explored. In this paper I propose a study to assess the predictive validity that these variables have on the abusive use of the smartphone. The technique I propose for the analysis of this relationship is OLS multiple linear regression.&lt;/p&gt;
&lt;p&gt;If you would like to know more about this project, you can read the paper (in Spanish)
&lt;a href=&#34;https://diego-hernandez-jimenez.github.io/web/projects/smartadTFG.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
     </item>
   
 </channel>
</rss>
